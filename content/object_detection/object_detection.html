<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>TinyML Made Easy - Object Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../content/kws_feature_eng/kws_feature_eng.html" rel="next">
<link href="../../content/image_classification/image_classification.html" rel="prev">
<link href="../../cover.jpg" rel="icon" type="image/jpeg">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML Made Easy</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Mjrovai/TinyML_Made_Easy_NiclaV_eBook" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="../../TinyML-Made-Easy.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="../../TinyML-Made-Easy.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../content/object_detection/object_detection.html">Object Detection</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../Acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_book.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/object_detection/object_detection.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../content/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../about_the_author.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the author</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#object-detection-versus-image-classification" id="toc-object-detection-versus-image-classification" class="nav-link" data-scroll-target="#object-detection-versus-image-classification">Object Detection versus Image Classification</a></li>
  <li><a href="#an-innovative-solution-for-object-detection-fomo" id="toc-an-innovative-solution-for-object-detection-fomo" class="nav-link" data-scroll-target="#an-innovative-solution-for-object-detection-fomo">An innovative solution for Object Detection: FOMO</a></li>
  </ul></li>
  <li><a href="#the-object-detection-project-goal" id="toc-the-object-detection-project-goal" class="nav-link" data-scroll-target="#the-object-detection-project-goal">The Object Detection Project Goal</a></li>
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection">Data Collection</a>
  <ul class="collapse">
  <li><a href="#collecting-dataset-with-openmv-ide" id="toc-collecting-dataset-with-openmv-ide" class="nav-link" data-scroll-target="#collecting-dataset-with-openmv-ide">Collecting Dataset with OpenMV IDE</a></li>
  </ul></li>
  <li><a href="#edge-impulse-studio" id="toc-edge-impulse-studio" class="nav-link" data-scroll-target="#edge-impulse-studio">Edge Impulse Studio</a>
  <ul class="collapse">
  <li><a href="#setup-the-project" id="toc-setup-the-project" class="nav-link" data-scroll-target="#setup-the-project">Setup the project</a></li>
  <li><a href="#uploading-the-unlabeled-data" id="toc-uploading-the-unlabeled-data" class="nav-link" data-scroll-target="#uploading-the-unlabeled-data">Uploading the unlabeled data</a></li>
  <li><a href="#labeling-the-dataset" id="toc-labeling-the-dataset" class="nav-link" data-scroll-target="#labeling-the-dataset">Labeling the Dataset</a></li>
  </ul></li>
  <li><a href="#the-impulse-design" id="toc-the-impulse-design" class="nav-link" data-scroll-target="#the-impulse-design">The Impulse Design</a>
  <ul class="collapse">
  <li><a href="#preprocessing-all-dataset" id="toc-preprocessing-all-dataset" class="nav-link" data-scroll-target="#preprocessing-all-dataset">Preprocessing all dataset</a></li>
  </ul></li>
  <li><a href="#model-design-training-and-test" id="toc-model-design-training-and-test" class="nav-link" data-scroll-target="#model-design-training-and-test">Model Design, Training, and Test</a>
  <ul class="collapse">
  <li><a href="#how-fomo-works" id="toc-how-fomo-works" class="nav-link" data-scroll-target="#how-fomo-works">How FOMO works?</a></li>
  <li><a href="#test-model-with-live-classification" id="toc-test-model-with-live-classification" class="nav-link" data-scroll-target="#test-model-with-live-classification">Test model with “Live Classification”</a></li>
  </ul></li>
  <li><a href="#deploying-the-model" id="toc-deploying-the-model" class="nav-link" data-scroll-target="#deploying-the-model">Deploying the Model</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_NiclaV_eBook/edit/main/content/object_detection/object_detection.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_NiclaV_eBook/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_NiclaV_eBook/blob/main/content/object_detection/object_detection.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Object Detection</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/jpg/obj_det_ini.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><em>DALL·E 3 Prompt: Cartoon in the style of the 1940s or 1950s showcasing a spacious industrial warehouse interior. A conveyor belt is prominently featured, carrying a mixture of toy wheels and boxes. The wheels are distinguishable with their bright yellow centers and black tires. The boxes are white cubes painted with alternating black and white patterns. At the end of the moving conveyor stands a retro-styled robot, equipped with tools and sensors, diligently classifying and counting the arriving wheels and boxes. The overall aesthetic is reminiscent of mid-century animation with bold lines and a classic color palette.</em></figcaption>
</figure>
</div>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This continuation of Image Classification on Nicla Vision is now exploring <strong>Object Detection</strong>.</p>
<p> <img src="./images/png/11-od.png" class="img-fluid" style="width:100.0%" data-fig-align="center"></p>
<section id="object-detection-versus-image-classification" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-versus-image-classification">Object Detection versus Image Classification</h3>
<p>The main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:</p>
<p> <img src="images/png/img_1.png" class="img-fluid" style="width:45.0%" data-fig-align="center"></p>
<p>But what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:</p>
<p> <img src="images/png/img_2.png" class="img-fluid" style="width:45.0%" data-fig-align="center"></p>
<p>And what happens if there is not a dominant category on the image?</p>
<p> <img src="images/png/img_3.png" class="img-fluid" style="width:60.0%" data-fig-align="center"></p>
<p>The model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.</p>
<blockquote class="blockquote">
<p>The model used in all previous examples is MobileNet, which was trained with a large dataset, <em>ImageNet</em>.</p>
</blockquote>
<p>To solve this issue, we need another type of model, where not only <strong>multiple categories</strong> (or labels) can be found but also <strong>where</strong> the objects are located on a given image.</p>
<p>As we can imagine, such models are much more complicated and bigger, for example, the <strong>MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset.</strong> This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:</p>
<p> <img src="images/png/img_4.png" class="img-fluid" style="width:65.0%" data-fig-align="center"></p>
<p>Those models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for Raspberry Pi but unsuitable for use with embedded devices, where the RAM is usually lower than 1 Mbyte.</p>
</section>
<section id="an-innovative-solution-for-object-detection-fomo" class="level3">
<h3 class="anchored" data-anchor-id="an-innovative-solution-for-object-detection-fomo">An innovative solution for Object Detection: FOMO</h3>
<p><a href="https://docs.edgeimpulse.com/docs/edge-impulse-studio/learning-blocks/object-detection/fomo-object-detection-for-constrained-devices">Edge Impulse launched in 2022, <strong>FOMO</strong> (Faster Objects, More Objects)</a>, a novel solution for performing object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) and the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).</p>
<p>In this Hands-On lab, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the <a href="https://www.edgeimpulse.com/blog/announcing-fomo-faster-objects-more-objects">official FOMO announcement</a> by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works.</p>
</section>
</section>
<section id="the-object-detection-project-goal" class="level2">
<h2 class="anchored" data-anchor-id="the-object-detection-project-goal">The Object Detection Project Goal</h2>
<p>All Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial facility and must sort and count <strong>wheels</strong> and special <strong>boxes</strong>.</p>
<p> <img src="images/jpg/proj_goal.jpg" class="img-fluid" style="width:90.0%" data-fig-align="center"></p>
<p>In other words, we should perform a multi-label classification, where each image can have three classes:</p>
<ul>
<li><p>Background (No objects)</p></li>
<li><p>Box</p></li>
<li><p>Wheel</p></li>
</ul>
<p>Here are some not labeled image samples that we should use to detect the objects (wheels and boxes):</p>
<p> <img src="images/jpg/samples.jpg" class="img-fluid" style="width:100.0%"></p>
<p>We are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.</p>
<p>We will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a <em>raw dataset</em> (not labeled) with images that contain the objects to be detected.</p>
</section>
<section id="data-collection" class="level2">
<h2 class="anchored" data-anchor-id="data-collection">Data Collection</h2>
<p>For image capturing, we can use:</p>
<ul>
<li>Web Serial Camera tool,</li>
<li>Edge Impulse Studio,</li>
<li>OpenMV IDE,</li>
<li>A smartphone.</li>
</ul>
<p>Here, we will use the <strong>OpenMV IDE</strong>.</p>
<section id="collecting-dataset-with-openmv-ide" class="level3">
<h3 class="anchored" data-anchor-id="collecting-dataset-with-openmv-ide">Collecting Dataset with OpenMV IDE</h3>
<p>First, we create a folder on the computer where the data will be saved, for example, “data.” Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:</p>
<p> <img src="images/jpg/data_folder.jpg" class="img-fluid" style="width:100.0%" data-fig-align="center"></p>
<p>Edge impulse suggests that the objects should be similar in size and not overlap for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.</p>
<blockquote class="blockquote">
<p>We will not create separate folders for our images because each contains multiple labels.</p>
</blockquote>
<p>Connect the Nicla Vision to the OpenMV IDE and run the <code>dataset_capture_script.py</code>. Clicking on the Capture Image button will start capturing images:</p>
<p> <img src="images/jpg/img_5.jpg" class="img-fluid" style="width:90.0%" data-fig-align="center"></p>
<p>We suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.</p>
<blockquote class="blockquote">
<p>The stored images use a QVGA frame size <span class="math inline">\(320\times 240\)</span> and RGB565 (color pixel format).</p>
</blockquote>
<p>After capturing your dataset, close the Dataset Editor Tool on the <code>Tools &gt; Dataset Editor</code>.</p>
</section>
</section>
<section id="edge-impulse-studio" class="level2">
<h2 class="anchored" data-anchor-id="edge-impulse-studio">Edge Impulse Studio</h2>
<section id="setup-the-project" class="level3">
<h3 class="anchored" data-anchor-id="setup-the-project">Setup the project</h3>
<p>Go to <a href="https://www.edgeimpulse.com/">Edge Impulse Studio,</a> enter your credentials at <strong>Login</strong> (or create an account), and start a new project.</p>
<blockquote class="blockquote">
<p>Here, you can clone the project developed for this hands-on: <a href="https://studio.edgeimpulse.com/public/292737/latest">NICLA_Vision_Object_Detection</a>.</p>
</blockquote>
<p>On the Project <code>Dashboard</code>, go to <strong>Project info</strong> and select <strong>Bounding boxes (object detection),</strong> and at the right-top of the page, select <code>Target</code>, <strong>Arduino Nicla Vision (Cortex-M7)</strong>.</p>
<p> <img src="./images/png/0-setup.png" class="img-fluid" style="width:95.0%" data-fig-align="center"></p>
</section>
<section id="uploading-the-unlabeled-data" class="level3">
<h3 class="anchored" data-anchor-id="uploading-the-unlabeled-data">Uploading the unlabeled data</h3>
<p>On Studio, go to the <code>Data acquisition</code> tab, and on the <code>UPLOAD DATA</code> section, upload from your computer files captured.</p>
<p> <img src="./images/png/1-data.png" class="img-fluid" style="width:95.0%" data-fig-align="center"></p>
<blockquote class="blockquote">
<p>You can leave for the Studio to split your data automatically between Train and Test or do it manually.</p>
</blockquote>
<p> <img src="./images/png/2-data.png" class="img-fluid" style="width:95.0%" data-fig-align="center"></p>
<p>All the unlabeled images (51) were uploaded, but they still need to be labeled appropriately before being used as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link <code>Labeling queue (51)</code>.</p>
<p>There are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):</p>
<ul>
<li>Using yolov5</li>
<li>Tracking objects between frames</li>
</ul>
<blockquote class="blockquote">
<p>Edge Impulse launched an <a href="https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/auto-labeler">auto-labeling feature</a> for Enterprise customers, easing labeling tasks in object detection projects.</p>
</blockquote>
<p>Ordinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of <code>tracking objects</code>. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, <em>partially</em> labeling the new ones (not all are correctly labeled).</p>
<blockquote class="blockquote">
<p>If you already have a labeled dataset containing bounding boxes, import your data using the EI uploader.</p>
</blockquote>
</section>
<section id="labeling-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="labeling-the-dataset">Labeling the Dataset</h3>
<p>Starting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click <strong>Save labels</strong> to advance to the next item.</p>
<p> <img src="./images/png/3-labeling-data.png" class="img-fluid"></p>
<p>Continue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:</p>
<p> <img src="images/jpg/img_11.jpg" class="img-fluid"></p>
<p>Next, review the labeled samples on the <code>Data acquisition</code> tab. If one of the labels is wrong, it can be edited using the <em><code>three dots</code></em> menu after the sample name:</p>
<p> <img src="./images/png/4-labeling-data.png" class="img-fluid"></p>
<p>We will be guided to replace the wrong label and correct the dataset.</p>
<p> <img src="images/jpg/img_13.jpg" class="img-fluid"></p>
</section>
</section>
<section id="the-impulse-design" class="level2">
<h2 class="anchored" data-anchor-id="the-impulse-design">The Impulse Design</h2>
<p>In this phase, we should define how to:</p>
<ul>
<li><p><strong>Pre-processing</strong> consists of resizing the individual images from <code>320 x 240</code> to <code>96 x 96</code> and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.</p></li>
<li><p><strong>Design a Model,</strong> in this case, “Object Detection.”</p></li>
</ul>
<p> <img src="./images/png/5-impulse.png" class="img-fluid"></p>
<section id="preprocessing-all-dataset" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing-all-dataset">Preprocessing all dataset</h3>
<p>In this section, select <strong>Color depth</strong> as <code>Grayscale</code>, suitable for use with FOMO models and Save <code>parameters</code>.</p>
<p> <img src="./images/png/6-pp.png" class="img-fluid"></p>
<p>The Studio moves automatically to the next section, <code>Generate features</code>, where all samples will be pre-processed, resulting in a dataset with individual <span class="math inline">\(96\times 96\times 1\)</span> images or 9,216 features.</p>
<p> <img src="./images/png/image-20250410123417108.png" class="img-fluid"></p>
<p>The feature explorer shows that all samples evidence a good separation after the feature generation.</p>
<blockquote class="blockquote">
<p>One of the samples (46) is apparently in the wrong space, but clicking on it confirms that the labeling is correct.</p>
</blockquote>
</section>
</section>
<section id="model-design-training-and-test" class="level2">
<h2 class="anchored" data-anchor-id="model-design-training-and-test">Model Design, Training, and Test</h2>
<p>We will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of <strong>background</strong> vs <strong>objects of interest</strong> (here, <em>boxes</em> and <em>wheels</em>).</p>
<p>FOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.</p>
<section id="how-fomo-works" class="level3">
<h3 class="anchored" data-anchor-id="how-fomo-works">How FOMO works?</h3>
<p>FOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be <span class="math inline">\(12\times 12\)</span> <span class="math inline">\((96/8=12)\)</span>. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as <em>background</em>). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.</p>
<p> <img src="images/png/img_17.png" class="img-fluid"></p>
<p>For training, we should select a pre-trained model. Let’s use the <strong><code>FOMO (Faster Objects, More Objects) MobileNetV2 0.35</code>.</strong> This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board since it has 1 MB of RAM and ROM.</p>
<p> <img src="./images/png/7-fomo.png" class="img-fluid" style="width:80.0%" data-fig-align="center"></p>
<p>Regarding the training hyper-parameters, the model will be trained with:</p>
<ul>
<li>Epochs: 60,</li>
<li>Batch size: 32</li>
<li>Learning Rate: 0.001.</li>
</ul>
<p>For validation during training, 20% of the dataset (<em>validation_dataset</em>) will be spared. For the remaining 80% (<em>train_dataset</em>), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.</p>
<p>As a result, the model ends with an F1 score of around 91% (validation) and 93% (test data).</p>
<blockquote class="blockquote">
<p>Note that FOMO automatically added a 3rd label background to the two previously defined (<em>box</em> and <em>wheel</em>).</p>
</blockquote>
<p> <img src="./images/png/8-train.png" class="img-fluid" style="width:80.0%" data-fig-align="center"></p>
<blockquote class="blockquote">
<p>In object detection tasks, accuracy is generally not the primary <a href="https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/">evaluation metric</a>. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.</p>
</blockquote>
</section>
<section id="test-model-with-live-classification" class="level3">
<h3 class="anchored" data-anchor-id="test-model-with-live-classification">Test model with “Live Classification”</h3>
<p>Since Edge Impulse officially supports the Nicla Vision, let’s connect it to the Studio. For that, follow the steps:</p>
<ul>
<li><p>Download the <a href="https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip">last EI Firmware</a> and unzip it.</p></li>
<li><p>Open the zip file on your computer and select the uploader related to your OS</p></li>
<li><p>Put the Nicla-Vision on Boot Mode, pressing the reset button twice.</p></li>
<li><p>Execute the specific batch code for your OS to upload the binary (<code>arduino-nicla-vision.bin</code>) to your board.</p></li>
</ul>
<p>Go to <code>Live classification</code> section at EI Studio, and using <em>webUSB,</em> connect your Nicla Vision:</p>
<p> <img src="./images/png/9-live.png" class="img-fluid"></p>
<p>Once connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.</p>
<p> <img src="./images/png/10-live.png" class="img-fluid" style="width:90.0%" data-fig-align="center"></p>
<p>One thing to note is that the model can produce false positives and negatives. This can be minimized by defining a proper <code>Confidence Threshold</code> (use the <code>three dots</code> menu for the setup). Try with 0.8 or more.</p>
</section>
</section>
<section id="deploying-the-model" class="level2">
<h2 class="anchored" data-anchor-id="deploying-the-model">Deploying the Model</h2>
<p>Select <code>OpenMV Firmware</code> on the Deploy Tab and press <code>[Build]</code>.</p>
<p> <img src="./images/png/12-deploy.png" class="img-fluid" style="width:90.0%" data-fig-align="center"></p>
<p>When you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option <code>Load a specific firmware</code> instead. Or go to `Tools &gt; Runs Boatloader (Load Firmware).</p>
<p> <img src="images/png/img_24.png" class="img-fluid" style="width:65.0%" data-fig-align="center"></p>
<p>You will find a ZIP file on your computer from the Studio. Open it:</p>
<p> <img src="images/png/img_23.png" class="img-fluid" style="width:85.0%" data-fig-align="center"></p>
<p>Load the .bin file to your board:</p>
<p> <img src="images/png/img_25.png" class="img-fluid" style="width:65.0%" data-fig-align="center"></p>
<p>After the download is finished, a pop-up message will be displayed. <code>Press OK</code>, and open the script <strong>ei_object_detection.py</strong> downloaded from the Studio.</p>
<blockquote class="blockquote">
<p>Note: If a Pop-up appears saying that the FW is out of date, press <code>[NO]</code>, to upgrade it.</p>
</blockquote>
<p>Before running the script, let’s change a few lines. Note that you can leave the window definition as <span class="math inline">\(240\times 240\)</span> and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sensor</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ml</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ml.utils <span class="im">import</span> NMS</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> image</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>sensor.reset()  <span class="co"># Reset and initialize the sensor.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>sensor.set_pixformat(sensor.RGB565)  <span class="co"># Set pixel format (RGB565or GRAYSCALE)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sensor.set_framesize(sensor.QVGA)  <span class="co"># Set frame size to QVGA (320x240)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sensor.skip_frames(time<span class="op">=</span><span class="dv">2000</span>)  <span class="co"># Let the camera adjust.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Redefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>min_confidence <span class="op">=</span> <span class="fl">0.8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Change if necessary, the color of the circles that will be used to display the detected object’s centroid for a better contrast.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>threshold_list <span class="op">=</span> [(math.ceil(min_confidence <span class="op">*</span> <span class="dv">255</span>), <span class="dv">255</span>)]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load built-in model</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ml.Model(<span class="st">"trained"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternatively, models can be loaded from the filesystem storage.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># model = ml.Model('&lt;object_detection_modelwork&gt;.tflite', load_to_fb=True)</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># labels = [line.rstrip('\n') for line in open("labels.txt")]</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [ <span class="co"># Add more colors if you are detecting more</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>           <span class="co"># than 7 types of classes at once.</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">255</span>, <span class="dv">255</span>,   <span class="dv">0</span>), <span class="co"># background: yellow (not used)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    (  <span class="dv">0</span>, <span class="dv">255</span>,   <span class="dv">0</span>), <span class="co"># cube: green</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">255</span>,   <span class="dv">0</span>,   <span class="dv">0</span>), <span class="co"># wheel: red</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    (  <span class="dv">0</span>,   <span class="dv">0</span>, <span class="dv">255</span>), <span class="co"># not used</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">255</span>,   <span class="dv">0</span>, <span class="dv">255</span>), <span class="co"># not used</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    (  <span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="co"># not used</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>), <span class="co"># not used</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Keep the remaining code as it is</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># FOMO outputs an image per class where each pixel in the image is the centroid of the trained</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># object. So, we will get those output images and then run find_blobs() on them to extract the</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># centroids. We will also run get_stats() on the detected blobs to determine their score.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The Non-Max-Supression (NMS) object then filters out overlapping detections and maps their</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># position in the output image back to the original input image. The function then returns a</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># list per class which each contain a list of (rect, score) tuples representing the detected</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># objects.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fomo_post_process(model, inputs, outputs):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    n, oh, ow, oc <span class="op">=</span> model.output_shape[<span class="dv">0</span>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    nms <span class="op">=</span> NMS(ow, oh, inputs[<span class="dv">0</span>].roi)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(oc):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> image.Image(outputs[<span class="dv">0</span>][<span class="dv">0</span>, :, :, i] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        blobs <span class="op">=</span> img.find_blobs(</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>            threshold_list, x_stride<span class="op">=</span><span class="dv">1</span>, area_threshold<span class="op">=</span><span class="dv">1</span>, pixels_threshold<span class="op">=</span><span class="dv">1</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> b <span class="kw">in</span> blobs:</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> b.rect()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            x, y, w, h <span class="op">=</span> rect</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            score <span class="op">=</span> (</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                img.get_statistics(thresholds<span class="op">=</span>threshold_list, roi<span class="op">=</span>rect).l_mean() <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            nms.add_bounding_box(x, y, x <span class="op">+</span> w, y <span class="op">+</span> h, score, i)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nms.get_bounding_boxes()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>clock <span class="op">=</span> time.clock()</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    clock.tick()</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> sensor.snapshot()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, detection_list <span class="kw">in</span> <span class="bu">enumerate</span>(model.predict([img], callback<span class="op">=</span>fomo_post_process)):</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span>  <span class="co"># background class</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(detection_list) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span>  <span class="co"># no detections for this class?</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"********** </span><span class="sc">%s</span><span class="st"> **********"</span> <span class="op">%</span> model.labels[i])</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (x, y, w, h), score <span class="kw">in</span> detection_list:</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>            center_x <span class="op">=</span> math.floor(x <span class="op">+</span> (w <span class="op">/</span> <span class="dv">2</span>))</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>            center_y <span class="op">=</span> math.floor(y <span class="op">+</span> (h <span class="op">/</span> <span class="dv">2</span>))</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"x </span><span class="sc">{</span>center_x<span class="sc">}</span><span class="ch">\t</span><span class="ss">y </span><span class="sc">{</span>center_y<span class="sc">}</span><span class="ch">\t</span><span class="ss">score </span><span class="sc">{</span>score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>            img.draw_circle((center_x, center_y, <span class="dv">12</span>), color<span class="op">=</span>colors[i])</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(clock.fps(), <span class="st">"fps"</span>, end<span class="op">=</span><span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and press the <code>green Play button</code> to run the code:</p>
<p> <img src="./images/png/13-inf.png" class="img-fluid"></p>
<p>From the camera’s view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window <span class="math inline">\((240\times 240)\)</span>.</p>
<blockquote class="blockquote">
<p>Be aware that the coordinate origin is in the upper left corner.</p>
</blockquote>
<p> <img src="images/jpg/img_27.jpg" class="img-fluid" style="width:75.0%" data-fig-align="center"></p>
<p>Note that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet or YOLO. For example, when running a MobileNetV2 SSD FPN-Lite <span class="math inline">\(320\times 320\)</span> model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)</p>
<p>Here is a short video showing the inference results: </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JbpoqRp3BbM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>FOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:</p>
<blockquote class="blockquote">
<p>FOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.</p>
</blockquote>
<p>Multiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices. This can be very useful on projects counting bees, for example.</p>
<p> <img src="images/jpg/img_28.jpg" class="img-fluid" style="width:70.0%" data-fig-align="center"></p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://studio.edgeimpulse.com/public/292737/latest">Edge Impulse Project</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../content/image_classification/image_classification.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Image Classification</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../content/kws_feature_eng/kws_feature_eng.html" class="pagination-link">
        <span class="nav-page-text">KWS Feature Engineering</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Written and edited by Prof.&nbsp;Marcelo Rovai (UNIFEI University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>