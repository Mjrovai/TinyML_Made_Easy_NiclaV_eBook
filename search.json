[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TinyML Made Easy",
    "section": "",
    "text": "Preface\nFinding the right info used to be the big issue for people involved in technical projects. Nowadays, there is a deluge of information, but it is not easy to determine what can and what cannot be trusted. A good pointer is to look for the credentials and the affiliation of the author of a document or that of the associated institutions. Since 1992 EsLaRed has been providing training in different aspects of Information Technologies in Latin America and the Caribbean, always striving to provide accurate and timely training materials, which have evolved accordingly to shifts in the technology focus. IoT and Machine Learning are poised to play a pivotal role, so the work of Professor Marcelo Rovai addressing Tiny Machine Learning is both timely and authoritative, in this rapidly changing field.\nTinyML Made Easy is exactly what the title means. Written in a clear and concise style, with emphasis on practical applications and examples, drawing from many years of experience and overwhelming enthusiasm in sharing his knowledge, there is no doubt that Marcelo’s work is a very significant contribution to this very interesting field. The knowledge acquired from the exercises will enable the readers to undertake other projects that might interest them.\nErmanno Pietrosemoli, President Fundación Escuela Latinoamericana de Redes\nNovember 2023."
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "We extend our deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML).\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has served as a beacon for our efforts from the very beginning.\nWe also thank Ermanno Petrosemoli for his meticulous review of the content in this material.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\nIllustrative images on the e-book and chapter’s covers generated by OpenAI’s DALL-E via ChatGPT"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Microcontrollers (MCUs) are cheap electronic components, usually with just a few kilobytes of RAM, and designed to consume small amounts of power. Today, MCUs can be found embedded in all residential, medical, automotive, and industrial devices. Over 40 billion microcontrollers are estimated to be marketed annually, and hundreds of billions are currently in service. But, curiously, these devices receive little attention because, many times, they are used just to replace functionalities that older electromechanical systems face in cars, washing machines, or remote controls.\nMore recently, with the era of IoT (Internet of Things), a significant part of these MCUs is generating “quintillions” of data, which, in their majority, are not used due to the high cost and complexity of their data transmission (bandwidth and latency).\nOn the other hand, in the last decades, we have witnessed the development of Machine Learning models (sub-area of Artificial Intelligence) trained with “tons” of data and powerful mainframes. But now, it suddenly becomes possible for “noisy” and complex signals, such as images, audio, or accelerometers, to extract meaning from the same through neural networks. More importantly, we can execute these models of neural networks in microcontrollers and sensors using very little energy and extract much more meaning from the data generated by these sensors, which we are currently ignoring.\n\nTinyML, a new area of applied AI, allows the extracting of “machine intelligence” from the physical world (where the data is generated).\n\nTinyML Made Ease is a foundational text designed to facilitate the understanding and application of Embedded Machine Learning, or TinyML. In an era where embedded devices boast ultra-low power consumption—measured in mere milliwatts—and machine learning frameworks such as TensorFlow Lite for Microcontrollers (TF Lite Micro) become increasingly tailored for embedded applications, the intersection of AI and IoT is rapidly expanding. This book demystifies the integration of AI capabilities into these devices, paving the way for a wide-reaching adoption of AI-enabled IoT, or “AioT.”"
  },
  {
    "objectID": "about_book.html",
    "href": "about_book.html",
    "title": "About this Book",
    "section": "",
    "text": "TinyML Made Easy, an open-access eBook, is an accessible resource for enthusiasts, professionals, and engineering students embarking on the transformative path of embedded machine learning (TinyML). This book offers a systematic introduction to integrating ML algorithms with microcontrollers, focusing on practicality and hands-on experiences.\nStudents are introduced to core concepts of TinyML through the setup and programming of the Arduino Nicla Vision, a state-of-the-art microcontroller designed for ML applications. The book breaks down complex ideas into understandable segments, ensuring foundational knowledge is built from the ground up.\nTrue to the engineering ethos of ‘learning by doing,’ each chapter focuses on a project that challenges students to apply their knowledge in real-world scenarios. The projects are designed to reinforce learning and stimulate innovation, covering:\n\nMicrocontroller setup and sensor tests,\nImage classification,\nDataset labeling and Object detection,\nAudio processing and keyword spotting,\nTime-series data and Digital Signal Processing,\nMotion classification and Anomaly detection.\n\nTinyML Made Easy provides detailed walkthroughs of industry-standard tools such as Arduino IDE, OpenMV IDE, and Edge Impulse Studio. Students will gain hands-on experience with data collection, pre-processing, model training, and deployment, equipping them with the technical skills sought in the embedded systems industry.\nAs TinyML is poised to be a driving force in the evolution of IoT and smart devices, students will emerge from this book with an introductory theoretical understanding and the practical skills necessary to contribute to and shape the future of technology.\nWith its straightforward language, clarity, and focus on experiential learning, TinyML Made Easy is more than just a book—it’s a comprehensive toolkit for anyone ready to delve into the world of embedded machine learning. It empowers them to move beyond the classroom and into the lab or field, and they are well-prepared to tackle the challenges and opportunities TinyML presents.\nThe content of this book is also part of the open book Machine Learning Systems, which we invite you to read.\n\n\nSupplementary Online Resources\nSetup Nicla Vision\n\nMicropython codes\nArduino Codes\n\nImage Classification\n\nMicropython codes\nDataset\nEdge Impulse Project\n\nObject Detection\n\nEdge Impulse Project\n\nAudio Feature Engineering\n\nAudio_Data_Analysis Colab Notebook\n\nKeyword Spotting (KWS)\n\nSubset of Google Speech Commands Dataset\nKWS_MFCC_Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino post-processing code\nEdge Impulse Project\n\nDSP Spectral Features\n\nDSP - Spectral Features Colab Notebook\n\nMotion Classification and Anomaly Detection)\n\nArduino Code\nEdge_Impulse_Spectral_Features_Block Colab Notebook\nEdge Impulse Project"
  },
  {
    "objectID": "content/setup/setup.html#overview",
    "href": "content/setup/setup.html#overview",
    "title": "Setup",
    "section": "Overview",
    "text": "Overview\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference). At the same time, the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the simultaneous management of WiFi and Bluetooth Low Energy (BLE) connectivity."
  },
  {
    "objectID": "content/setup/setup.html#hardware",
    "href": "content/setup/setup.html#hardware",
    "title": "Setup",
    "section": "Hardware",
    "text": "Hardware\n\nTwo Parallel Cores\nThe central processor is the dual-core STM32H747, including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm Mbed OS\nNative Mbed applications\nMicroPython / JavaScript via an interpreter\nTensorFlow Lite\n\n \n\n\nMemory\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1 MB, shared by both processors. This MCU also has incorporated 2 MB of FLASH, mainly for code storage.\n\n\nSensors\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low-power-ranging capabilities to Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera."
  },
  {
    "objectID": "content/setup/setup.html#arduino-ide-installation",
    "href": "content/setup/setup.html#arduino-ide-installation",
    "title": "Setup",
    "section": "Arduino IDE Installation",
    "text": "Arduino IDE Installation\nStart connecting the board (micro USB) to your computer:\n \nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n \nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\nTesting the Microphone\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open it, and run the sketch. Open the Plotter and see the audio representation from the microphone:\n \n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\nTesting the IMU\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. To do so, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n \nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n \n\n\nTesting the ToF (Time of Flight) Sensor\nAs we did with IMU, installing the VL53L1X ToF library is necessary. To do that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n \nNext, run the sketch proximity_detection.ino:\n \nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4 m).\n \n\n\nTesting the Camera\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but we can get the raw image data generated by the camera.\nWe can use the Web Serial Camera (API) to see the image generated by the camera. This web application streams the camera image over Web Serial from camera-equipped Arduino boards.\nThe Web Serial Camera example shows you how to send image data over the wire from your Arduino board and how to unpack the data in JavaScript for rendering. In addition, in the source code of the web application, we can find some example image filters that show us how to manipulate pixel data to achieve visual effects.\nThe Arduino sketch (CameraCaptureWebSerial) for sending the camera image data can be found here and is also directly available from the “Examples→Camera” menu in the Arduino IDE when selecting the Nicla board.\nThe web application for displaying the camera image can be accessed here. We may also look at [this tutorial, which explains the setup in more detail."
  },
  {
    "objectID": "content/setup/setup.html#installing-the-openmv-ide",
    "href": "content/setup/setup.html#installing-the-openmv-ide",
    "title": "Setup",
    "section": "Installing the OpenMV IDE",
    "text": "Installing the OpenMV IDE\nOpenMV IDE is the premier integrated development environment with OpenMV cameras, similar to the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n \nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n \nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\nUpdating the Bootloader\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM32H747_manageBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\n\nInstalling the Firmware\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n \nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n \nYou can leave the option Erase internal file system unselected and click [OK].\nNicla’s green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n \nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, “DFU firmware update complete!”. Press [OK].\n \nA green play button appears when the Nicla Vison connects to the Tool Bar.\n \nAlso, note that a drive named “NO NAME” will appear on your computer.\n \nEvery time you press the [RESET] button on the board, the main.py script stored on it automatically executes. You can load the main.py code on the IDE (File &gt; Open File...).\n \n\nThis code is the “Blink” code, confirming that the HW is OK.\n\n\n\nTesting the Camera\nTo test the camera, let’s run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (hellowolrd.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 27fps.\n \nHere is the helloworld.py script:\nimport sensor, time\n\nsensor.reset()                      # Reset and initialize\n                                    # the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565\n                                    # (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to\n                                    # QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take\n                                    # effect.\nclock = time.clock()                # Create a clock object\n                                    # to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return\n                                    # the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors."
  },
  {
    "objectID": "content/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "href": "content/setup/setup.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "title": "Setup",
    "section": "Connecting the Nicla Vision to Edge Impulse Studio",
    "text": "Connecting the Nicla Vision to Edge Impulse Studio\nWe will need the Edge Impulse Studio later in other labs. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, to start, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the Arduino CLI for your specific computer architecture (OS)\nDownload the most updated EI Firmware.\nUnzip both files and place all the files in the same folder.\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nRun the uploader (EI FW) corresponding to your OS:\n\n \n\nExecuting the specific batch code for your OS will upload the binary arduino-nicla-vision.bin to your board.\n\n\nUsing Chrome, WebUSB can be used to connect the Nicla to the EI Studio. The EI CLI is not needed.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n \nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab.\n \nFor example. IMU data (inercial):\n \nOr Image (Camera):\n \nYou can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the built-in microphone, the ToF (Proximity) or a combination of sensors (fusion)."
  },
  {
    "objectID": "content/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "href": "content/setup/setup.html#expanding-the-nicla-vision-board-optional",
    "title": "Setup",
    "section": "Expanding the Nicla Vision Board (optional)",
    "text": "Expanding the Nicla Vision Board (optional)\nA last item to explore is that sometimes, during prototyping, it is essential to experiment with external sensors and devices. An excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n \nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n \nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n \nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying “Hello Word” on the Serial Monitor. Here is the code.\n \nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver,\n# I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin “PC4” (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))   # create an analog object\n                                # from a pin\nval = adc.read()                # read an analog value\n\nwhile (True):\n\n    val = adc.read()\n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython."
  },
  {
    "objectID": "content/setup/setup.html#conclusion",
    "href": "content/setup/setup.html#conclusion",
    "title": "Setup",
    "section": "Conclusion",
    "text": "Conclusion\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the code used or commented on in this hands-on lab."
  },
  {
    "objectID": "content/setup/setup.html#resources",
    "href": "content/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nArduino Codes"
  },
  {
    "objectID": "content/image_classification/image_classification.html#overview",
    "href": "content/image_classification/image_classification.html#overview",
    "title": "Image Classification",
    "section": "Overview",
    "text": "Overview\nAs we initiate our studies into embedded machine learning or TinyML, it’s impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\n \nIn the “bullseye” of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML “Hello World”!\nThis lab will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow’s robust ecosystem, we’ll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe’ll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you’ll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board."
  },
  {
    "objectID": "content/image_classification/image_classification.html#computer-vision",
    "href": "content/image_classification/image_classification.html#computer-vision",
    "title": "Image Classification",
    "section": "Computer Vision",
    "text": "Computer Vision\nAt its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system’s ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n \nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification."
  },
  {
    "objectID": "content/image_classification/image_classification.html#image-classification-project-goal",
    "href": "content/image_classification/image_classification.html#image-classification-project-goal",
    "title": "Image Classification",
    "section": "Image Classification Project Goal",
    "text": "Image Classification Project Goal\nThe first step in any ML project is to define the goal. In this case, the goal is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). We will also collect images of a background where those two objects are absent."
  },
  {
    "objectID": "content/image_classification/image_classification.html#data-collection",
    "href": "content/image_classification/image_classification.html#data-collection",
    "title": "Image Classification",
    "section": "Data Collection",
    "text": "Data Collection\nOnce we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. For image capturing, we can use:\n\nWeb Serial Camera tool,\nEdge Impulse Studio,\nOpenMV IDE,\nA smartphone.\n\nHere, we will use the OpenMV IDE.\n\nCollecting Dataset with OpenMV IDE\nFirst, we should create a folder on our computer where the data will be saved, for example, “data.” Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n \nThe IDE will ask us to open the file where the data will be saved. Choose the “data” folder that was created. Note that new icons will appear on the Left panel.\n \nUsing the upper icon (1), enter with the first class name, for example, “periquito”:\n \nRunning the dataset_capture_script.py and clicking on the camera icon (2) will start capturing images:\n \nRepeat the same procedure with the other classes.\n \n\nWe suggest around 50 to 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of \\(320\\times 240\\) and the RGB565 (color pixel format).\nAfter capturing the dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nWe will end up with a dataset on our computer that contains three classes: periquito, robot, and background.\n \nWe should return to Edge Impulse Studio and upload the dataset to our created project."
  },
  {
    "objectID": "content/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "content/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Enter the account credentials and create a new project:\n \n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification."
  },
  {
    "objectID": "content/image_classification/image_classification.html#dataset",
    "href": "content/image_classification/image_classification.html#dataset",
    "title": "Image Classification",
    "section": "Dataset",
    "text": "Dataset\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n \nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be spared from the beginning and reserved for use only in the Test phase after training. The Validation Set will be used during training.\n\nThe EI Studio will take a percentage of training data to be used for validation\n\n \nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n \nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n \nRepeat the procedure for all three classes.\n\nSelecting a folder and upload all the files at once is possible.\n\nAt the end, you should see your “raw data” in the Studio:\n \nNote that when you start to upload the data, a pop-up window can appear, asking if you are building an Object Detection project. Select [NO].\n\n\n\n\n\nWe can always change it in the Dashboard section: One label per data item (Image Classification):\n\n\n\n\n\nOptionally, the Studio allows us to explore the data, showing a complete view of all the data in the project. We can clear, inspect, or change labels by clicking on individual data items. In our case, the data seems OK."
  },
  {
    "objectID": "content/image_classification/image_classification.html#the-impulse-design",
    "href": "content/image_classification/image_classification.html#the-impulse-design",
    "title": "Image Classification",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n \nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n \nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n \nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n \n\nImage Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 27,640 features \\((96\\times 96\\times 3)\\).\n \nPress [Save parameters] and Generate all features:\n \n\n\nModel Design\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter \\(\\alpha\\) (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier \\(\\alpha\\) is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (\\(96\\times 96\\) images) and V2 (\\(96\\times 96\\) or \\(160\\times 160\\) images), with several different \\(\\alpha\\) values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, \\(160\\times 160\\) images, and \\(\\alpha=1.0\\). Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3 MB RAM and 2.6 MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and \\(\\alpha=0.10\\) (around 53.2 K RAM and 101 K ROM).\n \nWe will use MobileNetV2 96x96 0.1 ( or 0.05) for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:"
  },
  {
    "objectID": "content/image_classification/image_classification.html#model-training",
    "href": "content/image_classification/image_classification.html#model-training",
    "title": "Image Classification",
    "section": "Model Training",
    "text": "Model Training\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n \nThe result is excellent, with 77 ms of latency (estimated), which should result in around 13 fps (frames per second) during inference."
  },
  {
    "objectID": "content/image_classification/image_classification.html#model-testing",
    "href": "content/image_classification/image_classification.html#model-testing",
    "title": "Image Classification",
    "section": "Model Testing",
    "text": "Model Testing\n \nNow, we should take the data set put aside at the start of the project and run the trained model using it as input:\n \nThe result is, again, excellent."
  },
  {
    "objectID": "content/image_classification/image_classification.html#deploying-the-model",
    "href": "content/image_classification/image_classification.html#deploying-the-model",
    "title": "Image Classification",
    "section": "Deploying the model",
    "text": "Deploying the model\nAt this point, we can deploy the trained model as a firmware (FW) and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n \n\nArduino Library\nFirst, Let’s deploy it as an Arduino Library:\n \nWe should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under the library name.\n\nNote that Arduino Nicla Vision has, by default, 512 KB of RAM allocated for the M7 core and an additional 244 KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86 ms of measured latency.\n \nHere is a short video showing the inference results: \n\n\nOpenMV\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let’s explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n \nOn the computer, we will find a ZIP file. Open it:\n \nUse the Bootloader tool on the OpenMV IDE to load the FW on your board (1):\n \nSelect the appropriate file (.bin for Nicla-Vision):\n \nAfter the download is finished, press OK:\n \nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n \nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n \nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n \nThe classification result will appear at the Serial Terminal. If it is dificult to read the result, include a new line in the code to add some delay:\nimport time\nWhile True:\n...\n    time.sleep_ms(200)  # Delay for .2 second\n\nChanging the Code to add labels\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24March25\n\nimport sensor\nimport time\nimport ml\n\nsensor.reset()  # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)  # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)  # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))  # Set 240x240 window.\nsensor.skip_frames(time=2000)  # Let the camera adjust.\n\nmodel = ml.Model(\"trained\")#mobilenet, load_to_fb=True)\n\nclock = time.clock()\n\nwhile True:\n    clock.tick()\n    img = sensor.snapshot()\n\n    fps = clock.fps()\n    lat = clock.avg()\n    print(\"**********\\nPrediction:\")\n    # Combines labels & confidence into a list of tuples and then \n    # sorts that list by the confidence values.\n    sorted_list = sorted(\n        zip(model.labels, model.predict([img])[0].flatten().tolist()), \n      key=lambda x: x[1], reverse=True\n    )\n  \n    # Print only the class with the highest probability\n    max_val = sorted_list[0][1]\n    max_lbl = sorted_list[0][0]\n    \n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    \n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw the label with the highest probability to the image viewer\n    img.draw_string(\n    10, 10,\n    max_lbl + \"\\n{:.2f}\".format(max_val),\n    mono_space = False,\n    scale=3\n    )\n    \n    time.sleep_ms(500)  # Delay for .5 second\nHere you can see the result:\n \nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference, the latency should drop to around 70 ms.\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\nPost-Processing with LEDs\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n \nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, time, ml\nfrom machine import LED\n\nledRed = LED(\"LED_RED\")\nledGre = LED(\"LED_GREEN\")\nledBlu = LED(\"LED_BLUE\")\n\nsensor.reset()  # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)  # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)  # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))  # Set 240x240 window.\nsensor.skip_frames(time=2000)  # Let the camera adjust.\n\nmodel = ml.Model(\"trained\")#mobilenet, load_to_fb=True)\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\nclock = time.clock()\n\ndef setLEDs(max_lbl):\n    if max_lbl == 'uncertain’:\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n    \n    if max_lbl == 'periquito’:\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot’:\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background’:\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n  while True:\n    img = sensor.snapshot()\n    \n    clock.tick()\n    fps = clock.fps()\n    lat = clock.avg()\n    print(\"**********\\nPrediction:\")\n    sorted_list = sorted(\n        zip(model.labels, model.predict([img])[0].flatten().tolist()), \n      key=lambda x: x[1], reverse=True\n    )\n  \n    # Print only the class with the highest probability\n    max_val = sorted_list[0][1]\n    max_lbl = sorted_list[0][0]\n    \n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    \n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw the label with the highest probability to the image viewer\n    img.draw_string(\n    10, 10,\n    max_lbl + \"\\n{:.2f}\".format(max_val),\n    mono_space = False,\n    scale=3\n    )\n    \n    setLEDs(max_lbl)\n    time.sleep_ms(200)  # Delay for .2 second\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n \nIn more detail"
  },
  {
    "objectID": "content/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "href": "content/image_classification/image_classification.html#image-classification-non-official-benchmark",
    "title": "Image Classification",
    "section": "Image Classification (non-official) Benchmark",
    "text": "Image Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n \nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino’s Library:"
  },
  {
    "objectID": "content/image_classification/image_classification.html#conclusion",
    "href": "content/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce."
  },
  {
    "objectID": "content/image_classification/image_classification.html#resources",
    "href": "content/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nMicropython codes\nDataset\nEdge Impulse Project"
  },
  {
    "objectID": "content/object_detection/object_detection.html#overview",
    "href": "content/object_detection/object_detection.html#overview",
    "title": "Object Detection",
    "section": "Overview",
    "text": "Overview\nThis continuation of Image Classification on Nicla Vision is now exploring Object Detection.\n \n\nObject Detection versus Image Classification\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n \nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n \nAnd what happens if there is not a dominant category on the image?\n \nThe model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in all previous examples is MobileNet, which was trained with a large dataset, ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n \nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for Raspberry Pi but unsuitable for use with embedded devices, where the RAM is usually lower than 1 Mbyte.\n\n\nAn innovative solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution for performing object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) and the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On lab, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works."
  },
  {
    "objectID": "content/object_detection/object_detection.html#the-object-detection-project-goal",
    "href": "content/object_detection/object_detection.html#the-object-detection-project-goal",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n \nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected."
  },
  {
    "objectID": "content/object_detection/object_detection.html#data-collection",
    "href": "content/object_detection/object_detection.html#data-collection",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nFor image capturing, we can use:\n\nWeb Serial Camera tool,\nEdge Impulse Studio,\nOpenMV IDE,\nA smartphone.\n\nHere, we will use the OpenMV IDE.\n\nCollecting Dataset with OpenMV IDE\nFirst, we create a folder on the computer where the data will be saved, for example, “data.” Next, on the OpenMV IDE, we go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n \nEdge impulse suggests that the objects should be similar in size and not overlap for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n \nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size \\(320\\times 240\\) and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor."
  },
  {
    "objectID": "content/object_detection/object_detection.html#edge-impulse-studio",
    "href": "content/object_detection/object_detection.html#edge-impulse-studio",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn the Project Dashboard, go to Project info and select Bounding boxes (object detection), and at the right-top of the page, select Target, Arduino Nicla Vision (Cortex-M7).\n \n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n \n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n \nAll the unlabeled images (51) were uploaded, but they still need to be labeled appropriately before being used as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nIf you already have a labeled dataset containing bounding boxes, import your data using the EI uploader.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n \nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n \nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, it can be edited using the three dots menu after the sample name:\n \nWe will be guided to replace the wrong label and correct the dataset."
  },
  {
    "objectID": "content/object_detection/object_detection.html#the-impulse-design",
    "href": "content/object_detection/object_detection.html#the-impulse-design",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, we should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n \n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual \\(96\\times 96\\times 1\\) images or 9,216 features.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) is apparently in the wrong space, but clicking on it confirms that the labeling is correct."
  },
  {
    "objectID": "content/object_detection/object_detection.html#model-design-training-and-test",
    "href": "content/object_detection/object_detection.html#model-design-training-and-test",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\n\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be \\(12\\times 12\\) \\((96/8=12)\\). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board since it has 1 MB of RAM and ROM.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an F1 score of around 91% (validation) and 93% (test data).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n\nTest model with “Live Classification”\nSince Edge Impulse officially supports the Nicla Vision, let’s connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS to upload the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n \nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n \nOne thing to note is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the three dots menu for the setup). Try with 0.8 or more."
  },
  {
    "objectID": "content/object_detection/object_detection.html#deploying-the-model",
    "href": "content/object_detection/object_detection.html#deploying-the-model",
    "title": "Object Detection",
    "section": "Deploying the Model",
    "text": "Deploying the Model\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n \nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead. Or go to `Tools &gt; Runs Boatloader (Load Firmware).\n \nYou will find a ZIP file on your computer from the Studio. Open it:\n \nLoad the .bin file to your board:\n \nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\n\nNote: If a Pop-up appears saying that the FW is out of date, press [NO], to upgrade it.\n\nBefore running the script, let’s change a few lines. Note that you can leave the window definition as \\(240\\times 240\\) and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\nimport sensor\nimport time\nimport ml\nfrom ml.utils import NMS\nimport math\nimport image\n\nsensor.reset()  # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)  # Set pixel format (RGB565or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)  # Set frame size to QVGA (320x240)\nsensor.skip_frames(time=2000)  # Let the camera adjust.\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object’s centroid for a better contrast.\nthreshold_list = [(math.ceil(min_confidence * 255), 255)]\n\n# Load built-in model\nmodel = ml.Model(\"trained\")\nprint(model)\n\n# Alternatively, models can be loaded from the filesystem storage.\n# model = ml.Model('&lt;object_detection_modelwork&gt;.tflite', load_to_fb=True)\n# labels = [line.rstrip('\\n') for line in open(\"labels.txt\")]\n\ncolors = [ # Add more colors if you are detecting more\n           # than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is\n# FOMO outputs an image per class where each pixel in the image is the centroid of the trained\n# object. So, we will get those output images and then run find_blobs() on them to extract the\n# centroids. We will also run get_stats() on the detected blobs to determine their score.\n# The Non-Max-Supression (NMS) object then filters out overlapping detections and maps their\n# position in the output image back to the original input image. The function then returns a\n# list per class which each contain a list of (rect, score) tuples representing the detected\n# objects.\n\ndef fomo_post_process(model, inputs, outputs):\n    n, oh, ow, oc = model.output_shape[0]\n    nms = NMS(ow, oh, inputs[0].roi)\n    for i in range(oc):\n        img = image.Image(outputs[0][0, :, :, i] * 255)\n        blobs = img.find_blobs(\n            threshold_list, x_stride=1, area_threshold=1, pixels_threshold=1\n        )\n        for b in blobs:\n            rect = b.rect()\n            x, y, w, h = rect\n            score = (\n                img.get_statistics(thresholds=threshold_list, roi=rect).l_mean() / 255.0\n            )\n            nms.add_bounding_box(x, y, x + w, y + h, score, i)\n    return nms.get_bounding_boxes()\n\n\nclock = time.clock()\nwhile True:\n    clock.tick()\n\n    img = sensor.snapshot()\n\n    for i, detection_list in enumerate(model.predict([img], callback=fomo_post_process)):\n        if i == 0:\n            continue  # background class\n        if len(detection_list) == 0:\n            continue  # no detections for this class?\n\n        print(\"********** %s **********\" % model.labels[i])\n        for (x, y, w, h), score in detection_list:\n            center_x = math.floor(x + (w / 2))\n            center_y = math.floor(y + (h / 2))\n            print(f\"x {center_x}\\ty {center_y}\\tscore {score}\")\n            img.draw_circle((center_x, center_y, 12), color=colors[i])\n\n    print(clock.fps(), \"fps\", end=\"\\n\")\nand press the green Play button to run the code:\n \nFrom the camera’s view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window \\((240\\times 240)\\).\n\nBe aware that the coordinate origin is in the upper left corner.\n\n \nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet or YOLO. For example, when running a MobileNetV2 SSD FPN-Lite \\(320\\times 320\\) model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:"
  },
  {
    "objectID": "content/object_detection/object_detection.html#conclusion",
    "href": "content/object_detection/object_detection.html#conclusion",
    "title": "Object Detection",
    "section": "Conclusion",
    "text": "Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices. This can be very useful on projects counting bees, for example."
  },
  {
    "objectID": "content/object_detection/object_detection.html#resources",
    "href": "content/object_detection/object_detection.html#resources",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project"
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#overview",
    "href": "content/kws_feature_eng/kws_feature_eng.html#overview",
    "title": "KWS Feature Engineering",
    "section": "Overview",
    "text": "Overview\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks"
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#the-kws",
    "href": "content/kws_feature_eng/kws_feature_eng.html#the-kws",
    "title": "KWS Feature Engineering",
    "section": "The KWS",
    "text": "The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition transcribes all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n \n\nApplications of KWS\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\nDifferences from General Speech Recognition\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#overview-to-audio-signals",
    "href": "content/kws_feature_eng/kws_feature_eng.html#overview-to-audio-signals",
    "title": "KWS Feature Engineering",
    "section": "Overview to Audio Signals",
    "text": "Overview to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16 kHz. Although music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n \n\nWhy Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#overview-to-mfccs",
    "href": "content/kws_feature_eng/kws_feature_eng.html#overview-to-mfccs",
    "title": "KWS Feature Engineering",
    "section": "Overview to MFCCs",
    "text": "Overview to MFCCs\n\nWhat are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n \n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\nWhy are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\nComputing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: \\(y(t)=x(t)-\\alpha x(t-1)\\), where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n \n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f)=|X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n \n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n \n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "href": "content/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "title": "KWS Feature Engineering",
    "section": "Hands-On using Python",
    "text": "Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]"
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#conclusion",
    "href": "content/kws_feature_eng/kws_feature_eng.html#conclusion",
    "title": "KWS Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\nMFCCs are particularly strong for\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\nSpectrograms or MFEs are often more suitable for\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#resources",
    "href": "content/kws_feature_eng/kws_feature_eng.html#resources",
    "title": "KWS Feature Engineering",
    "section": "Resources",
    "text": "Resources\n\nAudio_Data_Analysis Colab Notebook"
  },
  {
    "objectID": "content/kws/kws.html#overview",
    "href": "content/kws/kws.html#overview",
    "title": "Keyword Spotting (KWS)",
    "section": "Overview",
    "text": "Overview\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands."
  },
  {
    "objectID": "content/kws/kws.html#how-does-a-voice-assistant-work",
    "href": "content/kws/kws.html#how-does-a-voice-assistant-work",
    "title": "Keyword Spotting (KWS)",
    "section": "How does a voice assistant work?",
    "text": "How does a voice assistant work?\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as ” Hey Google” on the first one and “Alexa” on the second.\n \nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n \nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the TinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword."
  },
  {
    "objectID": "content/kws/kws.html#the-kws-hands-on-project",
    "href": "content/kws/kws.html#the-kws-hands-on-project",
    "title": "Keyword Spotting (KWS)",
    "section": "The KWS Hands-On Project",
    "text": "The KWS Hands-On Project\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n \nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOWN (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as “Noise” (or Background) and “Unknown.”\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):"
  },
  {
    "objectID": "content/kws/kws.html#dataset",
    "href": "content/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\nUploading the dataset to the Edge Impulse Studio\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n \nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n \nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n \n\n\nCapturing additional Audio Data\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16 KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16 KHz/16 bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n \n\nUsing the NiclaV and the Edge Impulse Studio\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n \n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n \nData on Pete’s dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n \nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\nUsing a smartphone and the EI Studio\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16 KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n \nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16 KHz/16-bit depth samples."
  },
  {
    "objectID": "content/kws/kws.html#creating-impulse-pre-process-model-definition",
    "href": "content/kws/kws.html#creating-impulse-pre-process-model-definition",
    "title": "Keyword Spotting (KWS)",
    "section": "Creating Impulse (Pre-Process / Model definition)",
    "text": "Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\nImpulse Design\n \nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500 ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, \\(13\\times 49\\times 1\\)). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\nPre-Processing (MFCC)\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n \nWe will take the Raw features (our 1-second, 16 KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 \\(\\times\\) 1 second), we will get 637 processed features \\((13\\times 49)\\).\n \nThe result shows that we only used a small amount of memory to pre-process data (16 KB) and a latency of 34 ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64 MHz), the same pre-process will take around 480 ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let’s Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n \nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\nGoing under the hood\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]"
  },
  {
    "objectID": "content/kws/kws.html#model-design-and-training",
    "href": "content/kws/kws.html#model-design-and-training",
    "title": "Keyword Spotting (KWS)",
    "section": "Model Design and Training",
    "text": "Model Design and Training\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n \nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n \n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n \nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as “yeh”. You can acquire additional samples and then retrain your model.\n\nGoing under the hood\nIf you want to understand what is happening “under the hood,” you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:"
  },
  {
    "objectID": "content/kws/kws.html#testing",
    "href": "content/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n \nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary “trigger” for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\nLive Classification\nWe can proceed to the project’s next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device."
  },
  {
    "objectID": "content/kws/kws.html#deploy-and-inference",
    "href": "content/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n \nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n \nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n \nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:"
  },
  {
    "objectID": "content/kws/kws.html#post-processing",
    "href": "content/kws/kws.html#post-processing",
    "title": "Keyword Spotting (KWS)",
    "section": "Post-processing",
    "text": "Post-processing\nNow that we know the model is working since it detects our keywords, let’s modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOWN, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let’s do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can\n        // comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n\n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n/*\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/*\n * @brief     turn_on_leds function used to turn on the RGB LEDs\n * @param[in] pred_index\n *            no:       [0] ==&gt; Red ON\n *            noise:    [1] ==&gt; ALL OFF\n *            unknown:  [2] ==&gt; Blue ON\n *            Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n\n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n     // print the predictions\n     ei_printf(\"Predictions \");\n     ei_printf(\"(DSP: %d ms., Classification: %d ms.,\n                 Anomaly: %d ms.)\",\n         result.timing.dsp, result.timing.classification,\n         result.timing.anomaly);\n     ei_printf(\": \\n\");\n     int pred_index = 0;     // Initialize pred_index\n     float pred_value = 0;   // Initialize pred_value\n     for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n         if (result.classification[ix].value &gt; pred_value){\n             pred_index = ix;\n             pred_value = result.classification[ix].value;\n         }\n         // ei_printf(\"    %s: \",\n         // result.classification[ix].label);\n         // ei_printf_float(result.classification[ix].value);\n         // ei_printf(\"\\n\");\n     }\n     ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\",\n               result.classification[pred_index].label,\n               pred_value);\n     turn_on_leds (pred_index);\n\n\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project’s GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can “trigger” an external device to perform a desired action instead of turning on an LED, as we saw in the introduction."
  },
  {
    "objectID": "content/kws/kws.html#conclusion",
    "href": "content/kws/kws.html#conclusion",
    "title": "Keyword Spotting (KWS)",
    "section": "Conclusion",
    "text": "Conclusion\n\nYou will find the notebooks and codeused in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)"
  },
  {
    "objectID": "content/kws/kws.html#resources",
    "href": "content/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nSubset of Google Speech Commands Dataset\nKWS MFCC Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino Post-processing Code\nEdge Impulse Project"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#overview",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#overview",
    "title": "DSP Spectral Features",
    "section": "Overview",
    "text": "Overview\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let’s dig into it."
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "title": "DSP Spectral Features",
    "section": "Extracting Features Review",
    "text": "Extracting Features Review\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations. Here’s a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It’s essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of “data cycles.”\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet’s explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons."
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "title": "DSP Spectral Features",
    "section": "A TinyML Motion Classification project",
    "text": "A TinyML Motion Classification project\n \nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n \nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50 Hz:\n \n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5 Hz instead of 50 Hz."
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "title": "DSP Spectral Features",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data captured by the accelerometer (a “time series” data) should be converted to “tabular data” using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 62.5 samples). The window is slid every 80 ms, creating a larger dataset where each instance has 375 “raw features.”\n \nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n \nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\nEdge Impulse - Spectral Analysis Block V.2 under the hood\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let’s choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n \nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n \nPaste the data points to a new variable data:\ndata = [\n    -5.6330,  0.2376,  9.8701,\n    -5.9442,  0.4830,  9.8701,\n    -5.4217, ...\n]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where \\(N= 125\\) (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n \nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [\n    2.7322, -0.0978, -0.3813,\n    2.3980, 3.8924, 24.6841,\n    9.6303, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let’s split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ]\nplot_data(sensors, axis, 'Raw Features')\n \nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [\n    (sum(x) / len(x))\n    for x in sensors\n]\n\n[\n    print('mean_' + x + ' =', round(y, 4))\n    for x, y in zip(axis, dtmean)\n][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "title": "DSP Spectral Features",
    "section": "Time Domain Statistical features",
    "text": "Time Domain Statistical features\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the “value of the direct current that dissipates the same power in a resistor.”\nIn the case of a set of \\(n\\) values \\({𝑥_1, 𝑥_2, \\ldots, 𝑥_𝑛}\\), the RMS is: \\[\nx_{\\mathrm{RMS}} = \\sqrt{\\frac{1}{n} \\left( x_1^2 + x_2^2 + \\cdots + x_n^2 \\right)}\n\\]\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standardized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n \nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n \n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n \n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "title": "DSP Spectral Features",
    "section": "Spectral features",
    "text": "Spectral features\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or “sub-windows”), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch’s method\nWe should use Welch’s method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len,\n        # and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n \nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet’s now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) # 13: 3+N_feat_axis;\n                           # 26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "title": "DSP Spectral Features",
    "section": "Time-frequency domain",
    "text": "Time-frequency domain\n\nWavelets\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet’s select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n \nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n \nAs we did before, let’s copy and past the Processed Features:\n \nfeatures = [\n    3.6251, 0.0615, 0.0615,\n    -7.3517, -2.7641, 2.8462,\n    5.0924, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statiscal Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline \\((y = 0)\\) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be \\(14\\times 2\\) (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\nWavelet Analysis\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the “Multilevel 1D Discrete Wavelet Transform”, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n \n\n\nFeature Extraction\nLet’s start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n\nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal’s frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\n        \"{:.2f}\".format(\n          (((my_array[:-1] * my_array[1:]) &lt; 0).sum()) / len(arr)\n        )\n    )\n    return zcross\n\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal’s uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet’s now list all the wavelet features and create a list by layers.\nL1_features_names = [\n    \"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\",\n    \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\",\n    \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"\n]\n\nL0_features_names = [\n    \"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\",\n    \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\",\n    \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"\n]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = (\n        stat_feat_l0[i]\n        + [skew_l0[i]]\n        + [kurtosis_l0[i]]\n        + [cross_l0[0][i]]\n        + [cross_l0[1][i]]\n        + [entropy_l0[i]]\n    )\n    [print(axis[i] + ' +x+= ', round(y, 4))\n       for x, y in zip(LO_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\n\nall_feat_l0 = [\n    item\n    for sublist in all_feat_l0\n    for item in sublist\n]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\n    feat_l1 = (\n        stat_feat_l1[i]\n        + [skew_l1[i]]\n        + [kurtosis_l1[i]]\n        + [cross_l1[0][i]]\n        + [cross_l1[1][i]]\n        + [entropy_l1[i]]\n    )\n    [print(axis[i]+' '+x+'= ', round(y, 4))\n       for x,y in zip(L1_features_names, feat_l1)][0]\n    all_feat_l1.append(feat_l1)\n\nall_feat_l1 = [\n    item\n    for sublist in all_feat_l1\n    for item in sublist\n]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "title": "DSP Spectral Features",
    "section": "Conclusion",
    "text": "Conclusion\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: “Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.” I recommend you read Dan’s excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#overview",
    "href": "content/motion_classification/motion_classification.html#overview",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Overview",
    "text": "Overview\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers’ safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\n\n\n\nBy the end of this tutorial, you’ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#imu-installation-and-testing",
    "href": "content/motion_classification/motion_classification.html#imu-installation-and-testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "IMU Installation and testing",
    "text": "IMU Installation and testing\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let’s verify if the LSM6DSOX IMU library is installed. If not, install it.\n \nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n \n\nDefining the Sampling frequency:\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you’re interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet’s define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50 Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines ---------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n\n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2);\n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2);\n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n \n\nNote that with the Nicla board resting on a table (with the camera facing down), the \\(z\\)-axis measures around 9.8 m/s\\(^2\\), the expected earth acceleration."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "href": "content/motion_classification/motion_classification.html#the-case-study-simulated-container-transportation",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The Case Study: Simulated Container Transportation",
    "text": "The Case Study: Simulated Container Transportation\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we’ll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n \nFrom the above images, we can define for our simulation that primarily horizontal movements (\\(x\\) or \\(y\\) axis) should be associated with the “Terrestrial class,” Vertical movements (\\(z\\)-axis) with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#data-collection",
    "href": "content/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\nConnecting the device to Edge Impulse\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let’s do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla’s accelerometer and send it to the Studio, as shown in this diagram:\n\n \nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n \nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n \n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\nData Collection\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50 Hz]. The Studio suggests a sample length of [10000] ms (10 s). The last thing left is defining the sample label. Let’s start with[terrestrial]:\n \nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n \nAs expected, the movement was captured mainly in the \\(Y\\)-axis (green). In the blue, we see the \\(Z\\) axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n \nLift (Palettes being handled vertically by a Forklift). Movement captured only in the \\(Z\\)-axis:\n \nIdle (Paletts in a warehouse). No movement detected by the accelerometer:\n \nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n \nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n \nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only “vertical”."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#impulse-design",
    "href": "content/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let’s also add a second Learning Block for Anomaly Detection.\n \nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n \nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n \nLet’s dig into those steps and parameters to understand better what we are doing here.\n\nData Pre-Processing Overview\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as \\(X\\), \\(Y\\), and \\(Z\\)). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of “cycles of data”.\n\nWith a sampling rate (SR) of 50 Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 50 samples). We will slide this window every 200 ms, creating a larger dataset where each instance has 300 raw features.\n\n \nOnce the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\nEI Studio Spectral Features\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\nGenerating features\nOnce we understand what the pre-processing does, it is time to finish the job. So, let’s take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n \nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#models-training",
    "href": "content/motion_classification/motion_classification.html#models-training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Models Training",
    "text": "Models Training\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n \nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:"
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#testing",
    "href": "content/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classify all button.\n \nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device)."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#deploy",
    "href": "content/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n \nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\nInference\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n \nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n \n\nMaritime and terrestrial:\n\n \nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, “rolling” the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nAnomaly detection:\n\n \nIn this case, the anomaly is much bigger, over 1.00\n\n\nPost-processing\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5 V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously)."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#conclusion",
    "href": "content/motion_classification/motion_classification.html#conclusion",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe notebooks and codeused in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\nCase Applications\n\nIndustrial and Manufacturing\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\nHealthcare\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\nConsumer Electronics\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\nTransportation and Logistics\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\nSmart Cities and Infrastructure\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\nSecurity and Surveillance\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\nAgriculture\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\nEnvironmental Monitoring\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\nNicla 3D case\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n \nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited."
  },
  {
    "objectID": "content/motion_classification/motion_classification.html#resources",
    "href": "content/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nArduino Code\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Project"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis” by Wes McKinney\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden and Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake and Jenny Plunkett\n“XIAO: Big Power, Small Board” by Lei Feng and Marcelo Rovai\n“Machine Learning Systems” by Vijay Janapa Reddi\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  },
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "Marcelo Rovai, a Brazilian living in Chile, is a recognized engineering and technology education figure. He holds the title of Professor Honoris Causa from the Federal University of Itajubá (UNIFEI), Brazil. His educational background includes an Engineering degree from UNIFEI and a specialization from the Polytechnic School of São Paulo University (POLI/USP). Further enhancing his expertise, he earned an MBA from IBMEC (INSPER) and a Master’s in Data Science from the Universidad del Desarrollo (UDD) in Chile.\nWith a career spanning several high-profile technology companies such as AVIBRAS Airspace, AT&T, NCR, and IGT, where he served as Vice President for Latin America, he brings industry experience to his academic endeavors. He is a prolific writer on electronics-related topics and shares his knowledge through open platforms like Hackster.io.\nIn addition to his professional pursuits, he is dedicated to educational outreach, serving as a volunteer professor at UNIFEI and engaging with the TinyML4D group and the EDGE AIP– the Academia-Industry Partnership of EDGEAI Foundation as a Co-Chair, promoting TinyML education in developing countries. His work underscores a commitment to leveraging technology for societal advancement.\nLinkedIn profile: https://www.linkedin.com/in/marcelo-jose-rovai-brazil-chile/\nLectures, books, papers, and tutorials: https://github.com/Mjrovai/TinyML4D"
  }
]