[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TinyML Made Easy",
    "section": "",
    "text": "Preface\nFinding the right info used to be the big issue for people involved in technical projects. Nowadays, there is a deluge of information, but it is not easy to determine what can and what cannot be trusted. A good pointer is to look for the credentials and the affiliation of the author of a document or that of the associated institutions. Since 1992 EsLaRed has been providing training in different aspects of Information Technologies in Latin America and the Caribbean, always striving to provide accurate and timely training materials, which have evolved accordingly to shifts in the technology focus. IoT and Machine Learning are poised to play a pivotal role, so the work of Professor Marcelo Rovai addressing Tiny Machine Learning is both timely and authoritative, in this rapidly changing field.\nTinyML Made Easy is exactly what the title means. Written in a clear and concise style, with emphasis on practical applications and examples, drawing from many years of experience and overwhelming enthusiasm in sharing his knowledge, there is no doubt that Marcelo’s work is a very significant contribution to this very interesting field. The knowledge acquired from the exercises will enable the readers to undertake other projects that might interest them.\nErmanno Pietrosemoli, President Fundación Escuela Latinoamericana de Redes\nNovember 2023."
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "We extend our deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML).\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has served as a beacon for our efforts from the very beginning.\nWe also thank Ermanno Petrosemoli for his meticulous review of the content in this material.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\nIllustrative images on the e-book and chapter’s covers generated by OpenAI’s DALL-E via ChatGPT"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Microcontrollers (MCUs) are cheap electronic components, usually with just a few kilobytes of RAM, and designed to consume small amounts of power. Today, MCUs can be found embedded in all residential, medical, automotive, and industrial devices. Over 40 billion microcontrollers are estimated to be marketed annually, and hundreds of billions are currently in service. But, curiously, these devices receive little attention because, many times, they are used just to replace functionalities that older electromechanical systems face in cars, washing machines, or remote controls.\nMore recently, with the era of IoT (Internet of Things), a significant part of these MCUs is generating “quintillions” of data, which, in their majority, are not used due to the high cost and complexity of their data transmission (bandwidth and latency).\nOn the other hand, in the last decades, we have witnessed the development of Machine Learning models (sub-area of Artificial Intelligence) trained with “tons” of data and powerful mainframes. But now, it suddenly becomes possible for “noisy” and complex signals, such as images, audio, or accelerometers, to extract meaning from the same through neural networks. More importantly, we can execute these models of neural networks in microcontrollers and sensors using very little energy and extract much more meaning from the data generated by these sensors, which we are currently ignoring.\n\nTinyML, a new area of Applied AI, allows extracting “machine intelligence” from the physical world (where the data is generated).\n\nTinyML Made Ease is a foundational text designed to facilitate the understanding and application of Embedded Machine Learning, or TinyML. In an era where embedded devices boast ultra-low power consumption—measured in mere milliwatts—and machine learning frameworks such as TensorFlow Lite for Microcontrollers (TF Lite Micro) become increasingly tailored for embedded applications, the intersection of AI and IoT is rapidly expanding. This book demystifies the integration of AI capabilities into these devices, paving the way for a wide-reaching adoption of AI-enabled IoT, or “AioT.”"
  },
  {
    "objectID": "about_book.html",
    "href": "about_book.html",
    "title": "About this Book",
    "section": "",
    "text": "TinyML Made Easy, an open-access eBook, is an accessible resource for enthusiasts, professionals, and engineering students embarking on the transformative path of embedded machine learning, or TinyML. This book offers a systematic introduction to integrating ML algorithms with microcontrollers, focusing on practicality and hands-on experiences.\nStudents are introduced to core concepts of TinyML through the setup and programming of the Arduino Nicla Vision, a state-of-the-art microcontroller designed for ML applications. The book breaks down complex ideas into understandable segments, ensuring foundational knowledge is built from the ground up.\nTrue to the engineering ethos of ‘learning by doing,’ each chapter focuses on a project that challenges students to apply their knowledge in real-world scenarios. The projects are designed to reinforce learning and stimulate innovation, covering:\n\nMicrocontroller setup and sensor tests,\nImage classification,\nDataset labeling and Object detection,\nAudio processing and keyword spotting,\nTime-series data and Digital Signal Processing,\nMotion classification and Anomaly detection.\n\nTinyML Made Easy provides detailed walkthroughs of industry-standard tools such as Arduino IDE, OpenMV IDE, and Edge Impulse Studio. Students will gain hands-on experience with data collection, pre-processing, model training, and deployment, equipping them with the technical skills sought in the embedded systems industry.\nAs TinyML is poised to be a driving force in the evolution of IoT and smart devices, students will emerge from this book with an introductory theoretical understanding and the practical skills necessary to contribute to and shape the future of technology.\nWith its straightforward language, clarity, and focus on experiential learning, TinyML Made Easy is more than just a book—it’s a comprehensive toolkit for anyone ready to delve into the world of embedded machine learning. It empowers them to move beyond the classroom and into the lab or field, well-prepared to tackle the challenges and opportunities presented by TinyML.\n\nSupplementary Online Resources\nChapter 1 - Setup Nicla Vision\n\nMicropython codes\nArduino Codes\n\nChapter 2 - CV on Nicla Vision\n\nMicropython codes\nDataset\nEdge Impulse Project\n\nChapter 3 - Object Detection\n\nEdge Impulse Project\n\nChapter 4 - Audio Feature Engineering\n\nAudio_Data_Analysis Colab Notebook\n\nChapter 5 - Keyword Spotting (KWS)\n\nSubset of Google Speech Commands Dataset\nKWS_MFCC_Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino post-processing code\nEdge Impulse Project\n\nChapter 6 - Motion Classification and Anomaly Detection)\n\nArduino Code\nEdge_Impulse_Spectral_Features_Block Colab Notebook\nEdge Impulse Project"
  },
  {
    "objectID": "niclav_sys.html#introduction",
    "href": "niclav_sys.html#introduction",
    "title": "1  Setup Nicla Vision",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThe Arduino Nicla Vision (sometimes called NiclaV) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the Nicla Sense ME and the Nicla Voice. The Niclas can efficiently run processes created with TensorFlow™ Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously."
  },
  {
    "objectID": "niclav_sys.html#hardware",
    "href": "niclav_sys.html#hardware",
    "title": "1  Setup Nicla Vision",
    "section": "1.2 Hardware",
    "text": "1.2 Hardware\n\n1.2.1 Two Parallel Cores\nThe central processor is the dual-core STM32H747, including a Cortex® M7 at 480 MHz and a Cortex® M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\nArduino sketches on top of the Arm® Mbed™ OS\nNative Mbed™ applications\nMicroPython / JavaScript via an interpreter\nTensorFlow™ Lite\n\n\n\n\n\n\n\n\n1.2.2 Memory\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n\n1.2.3 Sensors\n\nCamera: A GC2145 2 MP Color CMOS Camera.\nMicrophone: The MP34DT05 is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n6-Axis IMU: 3D gyroscope and 3D accelerometer data from the LSM6DSOX 6-axis IMU.\nTime of Flight Sensor: The VL53L1CBV0FY Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera."
  },
  {
    "objectID": "niclav_sys.html#arduino-ide-installation",
    "href": "niclav_sys.html#arduino-ide-installation",
    "title": "1  Setup Nicla Vision",
    "section": "1.3 Arduino IDE Installation",
    "text": "1.3 Arduino IDE Installation\nStart connecting the board (microUSB) to your computer:\n\n\n\n\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to Tools &gt; Board &gt; Board Manager, look for Arduino Nicla Vision on the search window, and install the board.\n\n\n\n\n\nNext, go to Tools &gt; Board &gt; Arduino Mbed OS Nicla Boards and select Arduino Nicla Vision. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\nOpen the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n\n1.3.1 Testing the Microphone\nOn Arduino IDE, go to Examples &gt; PDM &gt; PDMSerialPlotter, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n\n\n\n\n\nVary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n\n\n1.3.2 Testing the IMU\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\n\n\n\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test (you can also run Gyro and board temperature):\n\n\n\n\n\n\n\n1.3.3 Testing the ToF (Time of Flight) Sensor\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\n\n\n\n\nNext, run the sketch proximity_detection.ino:\n\n\n\n\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n\n\n\n\n\n\n1.3.4 Testing the Camera\nWe can also test the camera using, for example, the code provided on Examples &gt; Camera &gt; CameraCaptureRawBytes. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV."
  },
  {
    "objectID": "niclav_sys.html#installing-the-openmv-ide",
    "href": "niclav_sys.html#installing-the-openmv-ide",
    "title": "1  Setup Nicla Vision",
    "section": "1.4 Installing the OpenMV IDE",
    "text": "1.4 Installing the OpenMV IDE\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\nGo to the OpenMV IDE page, download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\n\n\n\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from Files &gt; Examples &gt; HelloWord &gt; helloword.py\n\n\n\n\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the Serial Terminal during run time. The image captured by a camera will be displayed in the Camera Viewer Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras and the Arduino Pro boards. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the Nicla Vision.\n\nBefore connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on Examples &gt; STM_32H747_System STM_32H747_updateBootloader. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\n\n\n\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select Install the latest release firmware (vX.Y.Z). This action will install the latest OpenMV firmware on the Nicla Vision.\n\n\n\n\n\nYou can leave the option Erase internal file system unselected and click [OK].\nNicla’s green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\n\n\n\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, “DFU firmware update complete!”. Press [OK].\n\n\n\n\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\n\n\n\n\nAlso, note that a drive named “NO NAME” will appear on your computer.:\n\n\n\n\n\nEvery time you press the [RESET] button on the board, it automatically executes the main.py script stored on it. You can load the main.py code on the IDE (File &gt; Open File...).\n\n\n\n\n\n\nThis code is the “Blink” code, confirming that the HW is OK.\n\nFor testing the camera, let’s run helloword_1.py. For that, select the script on File &gt; Examples &gt; HelloWorld &gt; helloword.py,\nWhen clicking the green play button, the MicroPython script (hellowolrd.py) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\n\n\n\n\nHere is the helloworld.py script:\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\nIn GitHub, you can find the Python scripts used here.\nThe code can be split into two parts:\n\nSetup: Where the libraries are imported, initialized and the variables are defined and initiated.\nLoop: (while loop) part of the code that runs continually. The image (img variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red [X] button.\n\nNote: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the GitHub, You can find other Python scripts. Try to test the onboard sensors."
  },
  {
    "objectID": "niclav_sys.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "href": "niclav_sys.html#connecting-the-nicla-vision-to-edge-impulse-studio",
    "title": "1  Setup Nicla Vision",
    "section": "1.5 Connecting the Nicla Vision to Edge Impulse Studio",
    "text": "1.5 Connecting the Nicla Vision to Edge Impulse Studio\nWe will need the Edge Impulse Studio later in other exercises. Edge Impulse is a leading development platform for machine learning on edge devices.\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\nDownload the most updated EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\n\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary arduino-nicla-vision.bin to your board.\n\nGo to your project on the Studio, and on the Data Acquisition tab, select WebUSB (1). A window will pop up; choose the option that shows that the Nicla is paired (2) and press [Connect] (3).\n\n\n\n\n\nIn the Collect Data section on the Data Acquisition tab, you can choose which sensor data to pick.\n\n\n\n\n\nFor example. IMU data:\n\n\n\n\n\nOr Image (Camera):\n\n\n\n\n\nAnd so on. You can also test an external sensor connected to the ADC (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF."
  },
  {
    "objectID": "niclav_sys.html#expanding-the-nicla-vision-board-optional",
    "href": "niclav_sys.html#expanding-the-nicla-vision-board-optional",
    "title": "1  Setup Nicla Vision",
    "section": "1.6 Expanding the Nicla Vision Board (optional)",
    "text": "1.6 Expanding the Nicla Vision Board (optional)\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the Arduino MKR Connector Carrier (Grove compatible).\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\nNote that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n\n\n\n\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\n\n\n\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\n\n\n\n\nThe Grove Light Sensor would be connected to one of the single Analog pins (A0/PC4), the LoRaWAN device to the UART, and the OLED to the I2C connector.\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying “Hello Word” on the Serial Monitor. Here is the code.\n\n\n\n\n\nBelow is the Hello World code to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in GitHub).\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\nFinally, here is a simple script to read the ADC value on pin “PC4” (Nicla pin A0):\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\nThe ADC can be used for other sensor variables, such as Temperature.\n\nNote that the above scripts (downloaded from Github) introduce only how to connect external devices with the Nicla Vision board using MicroPython."
  },
  {
    "objectID": "niclav_sys.html#conclusion",
    "href": "niclav_sys.html#conclusion",
    "title": "1  Setup Nicla Vision",
    "section": "1.7 Conclusion",
    "text": "1.7 Conclusion\nThe Arduino Nicla Vision is an excellent tiny device for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\nOn the GitHub repository, you will find the last version of all the codes used or commented on in this hands-on exercise."
  },
  {
    "objectID": "image_classification.html#introduction",
    "href": "image_classification.html#introduction",
    "title": "2  CV on Nicla Vision",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nAs we initiate our studies into embedded machine learning or tinyML, it’s impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\n\n\n\n\n\nIn the “bullseye” of the Radar is the Edge Computer Vision, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is Image Classification, a kind of ML “Hello World”!\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow’s robust ecosystem, we’ll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\nWe’ll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you’ll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board."
  },
  {
    "objectID": "image_classification.html#computer-vision",
    "href": "image_classification.html#computer-vision",
    "title": "2  CV on Nicla Vision",
    "section": "2.2 Computer Vision",
    "text": "2.2 Computer Vision\nAt its core, computer vision aims to enable machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system’s ability to understand, interpret, and react to visual stimuli.\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are Image Classification and Object Detection.\n\n\n\n\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification."
  },
  {
    "objectID": "image_classification.html#image-classification-project-goal",
    "href": "image_classification.html#image-classification-project-goal",
    "title": "2  CV on Nicla Vision",
    "section": "2.3 Image Classification Project Goal",
    "text": "2.3 Image Classification Project Goal\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent."
  },
  {
    "objectID": "image_classification.html#data-collection",
    "href": "image_classification.html#data-collection",
    "title": "2  CV on Nicla Vision",
    "section": "2.4 Data Collection",
    "text": "2.4 Data Collection\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\n2.4.1 Collecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\n\n\n\n\nThe IDE will ask you to open the file where your data will be saved and choose the “data” folder that was created. Note that new icons will appear on the Left panel.\n\n\n\n\n\nUsing the upper icon (1), enter with the first class name, for example, “periquito”:\n\n\n\n\n\nRunning the dataset_capture_script.py and clicking on the camera icon (2), will start capturing images:\n\n\n\n\n\nRepeat the same procedure with the other classes\n\n\n\n\n\n\nWe suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor.\nOn your computer, you will end with a dataset that contains three classes: periquito, robot, and background.\n\n\n\n\n\nYou should return to Edge Impulse Studio and upload the dataset to your project."
  },
  {
    "objectID": "image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "2  CV on Nicla Vision",
    "section": "2.5 Training the model with Edge Impulse Studio",
    "text": "2.5 Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n\n\n\n\n\nHere, you can clone a similar project: NICLA-Vision_Image_Classification."
  },
  {
    "objectID": "image_classification.html#dataset",
    "href": "image_classification.html#dataset",
    "title": "2  CV on Nicla Vision",
    "section": "2.6 Dataset",
    "text": "2.6 Dataset\nUsing the EI Studio (or Studio), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\n\n\n\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into Training, Validation, and Test. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\n\n\n\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\n\n\n\n\nLeave to the Studio the splitting of the original dataset into train and test and choose the label about that specific data:\n\n\n\n\n\nRepeat the procedure for all three classes. At the end, you should see your “raw data” in the Studio:\n\n\n\n\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK."
  },
  {
    "objectID": "image_classification.html#the-impulse-design",
    "href": "image_classification.html#the-impulse-design",
    "title": "2  CV on Nicla Vision",
    "section": "2.7 The Impulse Design",
    "text": "2.7 The Impulse Design\nIn this phase, we should define how to:\n\nPre-process our data, which consists of resizing the individual images and determining the color depth to use (be it RGB or Grayscale) and\nSpecify a Model, in this case, it will be the Transfer Learning (Images) to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n\n\n\n\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\n\n\n\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\n\n\n\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\nGo to the Impulse Design Tab and create the impulse, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n\n\n\n\n\n2.7.1 Image Pre-Processing\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\n\n\n\n\nPress [Save parameters] and Generate all features:\n\n\n\n\n\n\n\n2.7.2 Model Design\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched MobileNetV2: Inverted Residuals and Linear Bottlenecks.\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to enhance performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is that of thinning a network uniformly at each layer.\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\n\n\n\n\nWe will use MobileNetV2 96x96 0.1 for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:"
  },
  {
    "objectID": "image_classification.html#model-training",
    "href": "image_classification.html#model-training",
    "title": "2  CV on Nicla Vision",
    "section": "2.8 Model Training",
    "text": "2.8 Model Training\nAnother valuable technique to be used with Deep Learning is Data Augmentation. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\n\n\n\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference."
  },
  {
    "objectID": "image_classification.html#model-testing",
    "href": "image_classification.html#model-testing",
    "title": "2  CV on Nicla Vision",
    "section": "2.9 Model Testing",
    "text": "2.9 Model Testing\n\n\n\n\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\n\n\n\n\nThe result is, again, excellent."
  },
  {
    "objectID": "image_classification.html#deploying-the-model",
    "href": "image_classification.html#deploying-the-model",
    "title": "2  CV on Nicla Vision",
    "section": "2.10 Deploying the model",
    "text": "2.10 Deploying the model\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n\n\n\n\n\n2.10.1 Arduino Library\nFirst, Let’s deploy it as an Arduino Library:\n\n\n\n\n\nYou should install the library as.zip on the Arduino IDE and run the sketch nicla_vision_camera.ino available in Examples under your library name.\n\nNote that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (malloc_addblock((void*)0x30000000, 288 * 1024);).\n\nThe result is good, with 86ms of measured latency.\n\n\n\n\n\nHere is a short video showing the inference results: \n\n\n2.10.2 OpenMV\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\n\n\n\n\nRunning this model as a .tflite directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let’s explore this option.\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\n\n\n\n\nOn your computer, you will find a ZIP file. Open it:\n\n\n\n\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\n\n\n\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\n\n\n\n\nAfter the download is finished, press OK:\n\n\n\n\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select [NO].\n\n\n\n\n\nNow, open the script ei_image_classification.py that was downloaded from the Studio and the.bin file for the Nicla.\n\n\n\n\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n\n\n\n\n\n2.10.2.1 Changing the Code to add labels\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\nUpload the code from GitHub, or modify it as below:\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val &lt; 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nHere you can see the result:\n\n\n\n\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\n\n\n\n\nThe latency will drop to only 71 ms.\n\n\n\n\n\n\nThe NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n\n\n2.10.2.2 Post-Processing with LEDs\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\n\n\n\n\nTo accomplish that, we should upload the code from GitHub or change the last code to include the LEDs:\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val &gt; max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val &lt; 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==&gt; latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\nLed Red 0n: Uncertain (no class is over 0.8)\nLed Green 0n: Periquito &gt; 0.8\nLed Blue 0n: Robot &gt; 0.8\nAll LEDs Off: Background &gt; 0.8\n\nHere is the result:\n\n\n\n\n\nIn more detail"
  },
  {
    "objectID": "image_classification.html#image-classification-non-official-benchmark",
    "href": "image_classification.html#image-classification-non-official-benchmark",
    "title": "2  CV on Nicla Vision",
    "section": "2.11 Image Classification (non-official) Benchmark",
    "text": "2.11 Image Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (tinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\n\n\n\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino’s Library:"
  },
  {
    "objectID": "image_classification.html#conclusion",
    "href": "image_classification.html#conclusion",
    "title": "2  CV on Nicla Vision",
    "section": "2.12 Conclusion",
    "text": "2.12 Conclusion\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\nAutonomous Vehicles: Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\nHealthcare: Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\nRetail: Automated checkout systems that identify products as they pass through a scanner.\nSecurity and Surveillance: Facial recognition, anomaly detection, and object tracking in real-time video feeds.\nAugmented Reality: Object detection and classification to overlay digital information in the real world.\nIndustrial Automation: Visual inspection of products, predictive maintenance, and robot and drone guidance.\nAgriculture: Drone-based crop monitoring and automated harvesting.\nNatural Language Processing: Image captioning and visual question answering.\nGesture Recognition: For gaming, sign language translation, and human-machine interaction.\nContent Recommendation: Image-based recommendation systems in e-commerce."
  },
  {
    "objectID": "object_detection_fomo.html#introduction",
    "href": "object_detection_fomo.html#introduction",
    "title": "3  Object Detection",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThis is a continuation of CV on Nicla Vision, now exploring Object Detection on microcontrollers.\n\n\n\n\n\n\n3.1.1 Object Detection versus Image Classification\nThe main task with Image Classification models is to produce a list of the most probable object categories present on an image, for example, to identify a tabby cat just after his dinner:\n\n\n\n\n\nBut what happens when the cat jumps near the wine glass? The model still only recognizes the predominant category on the image, the tabby cat:\n\n\n\n\n\nAnd what happens if there is not a dominant category on the image?\n\n\n\n\n\nThe model identifies the above image completely wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in all previous examples is the MobileNet, trained with a large dataset, the ImageNet.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\n\n\n\n\nThose models used for Object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually is lower than 1M Bytes.\n\n\n3.1.2 An innovative solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, not only on the Nicla Vision (Cortex M7) but also on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM and XIAO ESP32S3 Sense).\nIn this Hands-On exercise, we will explore using FOMO with Object Detection, not entering many details about the model itself. To understand more about how the model works, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works."
  },
  {
    "objectID": "object_detection_fomo.html#the-object-detection-project-goal",
    "href": "object_detection_fomo.html#the-object-detection-project-goal",
    "title": "3  Object Detection",
    "section": "3.2 The Object Detection Project Goal",
    "text": "3.2 The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial facility and must sort and count wheels and special boxes.\n\n\n\n\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nBox\nWheel\n\nHere are some not labeled image samples that we should use to detect the objects (wheels and boxes):\n\n\n\n\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the Nicla Vision for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected."
  },
  {
    "objectID": "object_detection_fomo.html#data-collection",
    "href": "object_detection_fomo.html#data-collection",
    "title": "3  Object Detection",
    "section": "3.3 Data Collection",
    "text": "3.3 Data Collection\nWe can use the Edge Impulse Studio, the OpenMV IDE, your phone, or other devices for the image capture. Here, we will use again the OpenMV IDE for our purpose.\n\n3.3.1 Collecting Dataset with OpenMV IDE\nFirst, create in your computer a folder where your data will be saved, for example, “data.” Next, on the OpenMV IDE, go to Tools &gt; Dataset Editor and select New Dataset to start the dataset collection:\n\n\n\n\n\nEdge impulse suggests that the objects should be of similar size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try with mixed sizes and positions to see the result.\n\nWe will not create separate folders for our images because each contains multiple labels.\n\nConnect the Nicla Vision to the OpenMV IDE and run the dataset_capture_script.py. Clicking on the Capture Image button will start capturing images:\n\n\n\n\n\nWe suggest around 50 images mixing the objects and varying the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the Tools &gt; Dataset Editor."
  },
  {
    "objectID": "object_detection_fomo.html#edge-impulse-studio",
    "href": "object_detection_fomo.html#edge-impulse-studio",
    "title": "3  Object Detection",
    "section": "3.4 Edge Impulse Studio",
    "text": "3.4 Edge Impulse Studio\n\n3.4.1 Setup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\n\n\n\n\nHere, you can clone the project developed for this hands-on: NICLA_Vision_Object_Detection.\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Nicla Vision as your Target Device:\n\n\n\n\n\n\n\n3.4.2 Uploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload from your computer files captured.\n\n\n\n\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually.\n\n\n\n\n\n\nAll the not labeled images (51) were uploaded but they still need to be labeled appropriately before using them as a dataset in the project. The Studio has a tool for that purpose, which you can find in the link Labeling queue (51).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\n3.4.3 Labeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\n\n\n\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\n\n\n\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels was wrong, you can edit it using the three dots menu after the sample name:\n\n\n\n\n\nYou will be guided to replace the wrong label, correcting the dataset."
  },
  {
    "objectID": "object_detection_fomo.html#the-impulse-design",
    "href": "object_detection_fomo.html#the-impulse-design",
    "title": "3  Object Detection",
    "section": "3.5 The Impulse Design",
    "text": "3.5 The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterwards, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\n\n\n\n\n3.5.1 Preprocessing all dataset\nIn this section, select Color depth as Grayscale, which is suitable for use with FOMO models and Save parameters.\n\n\n\n\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\n\n\n\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nOne of the samples (46) apparently is in the wrong space, but clicking on it can confirm that the labeling is correct."
  },
  {
    "objectID": "object_detection_fomo.html#model-design-training-and-test",
    "href": "object_detection_fomo.html#model-design-training-and-test",
    "title": "3  Object Detection",
    "section": "3.6 Model Design, Training, and Test",
    "text": "3.6 Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image, by means of its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions which have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\n\n\n\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35`. This model uses around 250KB RAM and 80KB of ROM (Flash), which suits well with our board since it has 1MB of RAM and ROM.\n\n\n\n\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60,\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with practically 1.00 in the F1 score, with a similar result when using the Test data.\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\n\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n3.6.1 Test model with “Live Classification”\nSince Edge Impulse officially supports the Nicla Vision, let’s connect it to the Studio. For that, follow the steps:\n\nDownload the last EI Firmware and unzip it.\nOpen the zip file on your computer and select the uploader related to your OS:\n\n\n\n\n\n\n\nPut the Nicla-Vision on Boot Mode, pressing the reset button twice.\nExecute the specific batch code for your OS for uploading the binary (arduino-nicla-vision.bin) to your board.\n\nGo to Live classification section at EI Studio, and using webUSB, connect your Nicla Vision:\n\n\n\n\n\nOnce connected, you can use the Nicla to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\n\n\n\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the set-up). Try with 0.8 or more."
  },
  {
    "objectID": "object_detection_fomo.html#deploying-the-model",
    "href": "object_detection_fomo.html#deploying-the-model",
    "title": "3  Object Detection",
    "section": "3.7 Deploying the Model",
    "text": "3.7 Deploying the Model\nSelect OpenMV Firmware on the Deploy Tab and press [Build].\n\n\n\n\n\nWhen you try to connect the Nicla with the OpenMV IDE again, it will try to update its FW. Choose the option Load a specific firmware instead.\n\n\n\n\n\nYou will find a ZIP file on your computer from the Studio. Open it:\n\n\n\n\n\nLoad the .bin file to your board:\n\n\n\n\n\nAfter the download is finished, a pop-up message will be displayed. Press OK, and open the script ei_object_detection.py downloaded from the Studio.\nBefore running the script, let’s change a few lines. Note that you can leave the window definition as 240 x 240 and the camera capturing images as QVGA/RGB. The captured image will be pre-processed by the FW deployed from Edge Impulse\n# Edge Impulse - OpenMV Object Detection Example\n\nimport sensor, image, time, os, tf, math, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\nRedefine the minimum confidence, for example, to 0.8 to minimize false positives and negatives.\nmin_confidence = 0.8\nChange if necessary, the color of the circles that will be used to display the detected object’s centroid for a better contrast.\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\ncolors = [ # Add more colors if you are detecting more than 7 types of classes at once.\n    (255, 255,   0), # background: yellow (not used)\n    (  0, 255,   0), # cube: green\n    (255,   0,   0), # wheel: red\n    (  0,   0, 255), # not used\n    (255,   0, 255), # not used\n    (  0, 255, 255), # not used\n    (255, 255, 255), # not used\n]\nKeep the remaining code as it is and press the green Play button to run the code:\n\n\n\n\n\nOn the camera view, we can see the objects with their centroids marked with 12 pixel-fixed circles (each circle has a distinct color, depending on its class). On the Serial Terminal, the model shows the labels detected and their position on the image window (240X240).\n\nBe ware that the coordinate origin is in the upper left corner.\n\n\n\n\n\n\nNote that the frames per second rate is around 8 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around 5 times higher (around 1.5 fps)\nHere is a short video showing the inference results:"
  },
  {
    "objectID": "object_detection_fomo.html#conclusion",
    "href": "object_detection_fomo.html#conclusion",
    "title": "3  Object Detection",
    "section": "3.8 Conclusion",
    "text": "3.8 Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices, for example, to explore the Nicla doing sensor fusion (camera + microphone) and object detection. This can be very useful on projects involving bees, for example."
  },
  {
    "objectID": "kws_feature_eng.html#introduction",
    "href": "kws_feature_eng.html#introduction",
    "title": "4  Audio Feature Engineering",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks"
  },
  {
    "objectID": "kws_feature_eng.html#the-kws",
    "href": "kws_feature_eng.html#the-kws",
    "title": "4  Audio Feature Engineering",
    "section": "4.2 The KWS",
    "text": "4.2 The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition aims to transcribe all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n\n\n\n\n\n\n4.2.0.1 Applications of KWS:\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\n4.2.0.2 Differences from General Speech Recognition:\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately."
  },
  {
    "objectID": "kws_feature_eng.html#introduction-to-audio-signals",
    "href": "kws_feature_eng.html#introduction-to-audio-signals",
    "title": "4  Audio Feature Engineering",
    "section": "4.3 Introduction to Audio Signals",
    "text": "4.3 Introduction to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16KHz. Altough music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n\n\n\n\n\n\n4.3.1 Why Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning."
  },
  {
    "objectID": "kws_feature_eng.html#introduction-to-mfccs",
    "href": "kws_feature_eng.html#introduction-to-mfccs",
    "title": "4  Audio Feature Engineering",
    "section": "4.4 Introduction to MFCCs",
    "text": "4.4 Introduction to MFCCs\n\n4.4.1 What are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n\n\n\n\n\n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\n4.4.2 Why are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\n4.4.3 Computing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: y(t) = x(t) - \\(\\alpha\\) x(t-1) , where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n\n\n\n\n\n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f) = |X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n\n\n\n\n\n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n\n\n\n\n\n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector."
  },
  {
    "objectID": "kws_feature_eng.html#hands-on-using-python",
    "href": "kws_feature_eng.html#hands-on-using-python",
    "title": "4  Audio Feature Engineering",
    "section": "4.5 Hands-On using Python",
    "text": "4.5 Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]"
  },
  {
    "objectID": "kws_feature_eng.html#conclusion",
    "href": "kws_feature_eng.html#conclusion",
    "title": "4  Audio Feature Engineering",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\n\n4.6.1 What Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\n4.6.1.1 MFCCs are particularly strong for:\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\n4.6.1.2 Spectrograms or MFEs are often more suitable for:\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts."
  },
  {
    "objectID": "kws_nicla.html#introduction",
    "href": "kws_nicla.html#introduction",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nHaving already explored the Nicla Vision board in the Image Classification and Object Detection applications, we are now shifting our focus to voice-activated applications with a project on Keyword Spotting (KWS).\nAs introduced in the Feature Engineering for Audio Classification Hands-On tutorial, Keyword Spotting (KWS) is integrated into many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and feasible on smaller, low-power devices. This tutorial will guide you through implementing a KWS system using TinyML on the Nicla Vision development board equipped with a digital microphone.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions, bringing them to life with voice-activated commands."
  },
  {
    "objectID": "kws_nicla.html#how-does-a-voice-assistant-work",
    "href": "kws_nicla.html#how-does-a-voice-assistant-work",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.2 How does a voice assistant work?",
    "text": "5.2 How does a voice assistant work?\nAs said, voice assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as ” Hey Google” on the first one and “Alexa” on the second.\n\n\n\n\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\n\n\n\n\nStage 1: A small microprocessor inside the Echo Dot or Google Home continuously listens, waiting for the keyword to be spotted, using a TinyML model at the edge (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example of a Google Assistant being programmed on a Raspberry Pi (Stage 2), with an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\nTo explore the above Google Assistant project, please see the tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this KWS project, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the Nicla Vision, which has a digital microphone that will be used to spot the keyword."
  },
  {
    "objectID": "kws_nicla.html#the-kws-hands-on-project",
    "href": "kws_nicla.html#the-kws-hands-on-project",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.3 The KWS Hands-On Project",
    "text": "5.3 The KWS Hands-On Project\nThe diagram below gives an idea of how the final KWS application should work (during inference):\n\n\n\n\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no words spoken; only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nFor real-world projects, it is always advisable to include other sounds besides the keywords, such as “Noise” (or Background) and “Unknown.”\n\n\n5.3.1 The Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):"
  },
  {
    "objectID": "kws_nicla.html#dataset",
    "href": "kws_nicla.html#dataset",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.4 Dataset",
    "text": "5.4 Dataset\nThe critical component of any Machine Learning Workflow is the dataset. Once we have decided on specific keywords, in our case (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In words such as yes and no, we can get 1,500 samples.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file to a location of your choice.\n\n\n5.4.1 Uploading the dataset to the Edge Impulse Studio\nInitiate a new project at Edge Impulse Studio (EIS) and select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\n\n\n\n\nDefine the Label, select Automatically split between train and test, and Upload data to the EIS. Repeat for all classes.\n\n\n\n\n\nThe dataset will now appear in the Data acquisition section. Note that the approximately 6,000 samples (1,500 for each class) are split into Train (4,800) and Test (1,200) sets.\n\n\n\n\n\n\n\n5.4.2 Capturing additional Audio Data\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor is essential. In the case of sound, this is optional because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is the type of energy. Sound is mechanical perturbation (longitudinal sound waves) that propagate through a medium, causing variations of pressure in it. Audio is an electrical (analog or digital) signal representing sound.\n\nWhen we pronounce a keyword, the sound waves should be converted to audio data. The conversion should be done by sampling the signal generated by the microphone at a 16KHz frequency with 16-bit per sample amplitude.\nSo, any device that can generate audio data with this basic specification (16KHz/16bits) will work fine. As a device, we can use the NiclaV, a computer, or even your mobile phone.\n\n\n\n\n\n\n5.4.2.1 Using the NiclaV and the Edge Impulse Studio\nAs we learned in the chapter Setup Nicla Vision, EIS officially supports the Nicla Vision, which simplifies the capture of the data from its sensors, including the microphone. So, please create a new project on EIS and connect the Nicla to it, following these steps:\n\nDownload the last updated EIS Firmware and unzip it.\nOpen the zip file on your computer and select the uploader corresponding to your OS:\n\n\n\n\n\n\n\nPut the NiclaV in Boot Mode by pressing the reset button twice.\n\n\n\n\n\n\n\nUpload the binary arduino-nicla-vision.bin to your board by running the batch code corresponding to your OS.\n\nGo to your project on EIS, and on the Data Acquisition tab, select WebUSB. A window will pop up; choose the option that shows that the Nicla is paired and press [Connect].\nYou can choose which sensor data to pick in the Collect Data section on the Data Acquisition tab. Select: Built-in microphone, define your label (for example, yes), the sampling Frequency[16000Hz], and the Sample length (in milliseconds), for example [10s]. Start sampling.\n\n\n\n\n\nData on Pete’s dataset have a length of 1s, but the recorded samples are 10s long and must be split into 1s samples. Click on three dots after the sample name and select Split sample.\nA window will pop up with the Split tool.\n\n\n\n\n\nOnce inside the tool, split the data into 1-second (1000 ms) records. If necessary, add or remove segments. This procedure should be repeated for all new samples.\n\n\n5.4.2.2 Using a smartphone and the EI Studio\nYou can also use your PC or smartphone to capture audio data, using a sampling frequency of 16KHz and a bit depth of 16.\nGo to Devices, scan the QR Code using your phone, and click on the link. A data Collection app will appear in your browser. Select Collecting Audio, and define your Label, data capture Length, and Category.\n\n\n\n\n\nRepeat the same procedure used with the NiclaV.\n\nNote that any app, such as Audacity, can be used for audio recording, provided you use 16KHz/16-bit depth samples."
  },
  {
    "objectID": "kws_nicla.html#creating-impulse-pre-process-model-definition",
    "href": "kws_nicla.html#creating-impulse-pre-process-model-definition",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.5 Creating Impulse (Pre-Process / Model definition)",
    "text": "5.5 Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\n5.5.1 Impulse Design\n\n\n\n\n\nFirst, we will take the data points with a 1-second window, augmenting the data and sliding that window in 500ms intervals. Note that the option zero-pad data is set. It is essential to fill with ‘zeros’ samples smaller than 1 second (in some cases, some samples can result smaller than the 1000 ms window on the split tool to avoid noise and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). As discussed in the Feature Engineering for Audio Classification Hands-On tutorial, we will use Audio (MFCC), which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are well suited for the human voice, our case here.\nNext, we select the Classification block to build our model from scratch using a Convolution Neural Network (CNN).\n\nAlternatively, you can use the Transfer Learning (Keyword Spotting) block, which fine-tunes a pre-trained keyword spotting model on your data. This approach has good performance with relatively small keyword datasets.\n\n\n\n5.5.2 Pre-Processing (MFCC)\nThe following step is to create the features to be trained in the next phase:\nWe could keep the default parameter values, but we will use the DSP Autotune parameters option.\n\n\n\n\n\nWe will take the Raw features (our 1-second, 16KHz sampled audio data) and use the MFCC processing block to calculate the Processed features. For every 16,000 raw features (16,000 x 1 second), we will get 637 processed features (13 x 49).\n\n\n\n\n\nThe result shows that we only used a small amount of memory to pre-process data (16KB) and a latency of 34ms, which is excellent. For example, on an Arduino Nano (Cortex-M4f @ 64MHz), the same pre-process will take around 480ms. The parameters chosen, such as the FFT length [512], will significantly impact the latency.\nNow, let’s Save parameters and move to the Generated features tab, where the actual features will be generated. Using UMAP, a dimension reduction technique, the Feature explorer shows how the features are distributed on a two-dimensional plot.\n\n\n\n\n\nThe result seems OK, with a visually clear separation between yes features (in red) and no features (in blue). The unknown features seem nearer to the no space than the yes. This suggests that the keyword no has more propensity to false positives.\n\n\n5.5.3 Going under the hood\nTo understand better how the raw sound is preprocessed, look at the Feature Engineering for Audio Classification chapter. You can play with the MFCC features generation by downloading this notebook from GitHub or [Opening it In Colab]"
  },
  {
    "objectID": "kws_nicla.html#model-design-and-training",
    "href": "kws_nicla.html#model-design-and-training",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.6 Model Design and Training",
    "text": "5.6 Model Design and Training\nWe will use a simple Convolution Neural Network (CNN) model, tested with 1D and 2D convolutions. The basic architecture has two blocks of Convolution + MaxPooling ([8] and [16] filters, respectively) and a Dropout of [0.25] for the 1D and [0.5] for the 2D. For the last layer, after Flattening, we have [4] neurons, one for each class:\n\n\n\n\n\nAs hyper-parameters, we will have a Learning Rate of [0.005] and a model trained by [100] epochs. We will also include a data augmentation method based on SpecAugment. We trained the 1D and the 2D models with the same hyperparameters. The 1D architecture had a better overall result (90.5% accuracy when compared with 88% of the 2D, so we will use the 1D.\n\n\n\n\n\n\nUsing 1D convolutions is more efficient because it requires fewer parameters than 2D convolutions, making them more suitable for resource-constrained environments.\n\nIt is also interesting to pay attention to the 1D Confusion Matrix. The F1 Score for yes is 95%, and for no, 91%. That was expected by what we saw with the Feature Explorer (no and unknown at close distance). In trying to improve the result, you can inspect closely the results of the samples with an error.\n\n\n\n\n\nListen to the samples that went wrong. For example, for yes, most of the mistakes were related to a yes pronounced as “yeh”. You can acquire additional samples and then retrain your model.\n\n5.6.1 Going under the hood\nIf you want to understand what is happening “under the hood,” you can download the pre-processed dataset (MFCC training data) from the Dashboard tab and run this Jupyter Notebook, playing with the code or [Opening it In Colab]. For example, you can analyze the accuracy by each epoch:"
  },
  {
    "objectID": "kws_nicla.html#testing",
    "href": "kws_nicla.html#testing",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.7 Testing",
    "text": "5.7 Testing\nTesting the model with the data reserved for training (Test Data), we got an accuracy of approximately 76%.\n\n\n\n\n\nInspecting the F1 score, we can see that for YES, we got 0.90, an excellent result since we expect to use this keyword as the primary “trigger” for our KWS project. The worst result (0.70) is for UNKNOWN, which is OK.\nFor NO, we got 0.72, which was expected, but to improve this result, we can move the samples that were not correctly classified to the training dataset and then repeat the training process.\n\n5.7.1 Live Classification\nWe can proceed to the project’s next step but also consider that it is possible to perform Live Classification using the NiclaV or a smartphone to capture live samples, testing the trained model before deployment on our device."
  },
  {
    "objectID": "kws_nicla.html#deploy-and-inference",
    "href": "kws_nicla.html#deploy-and-inference",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.8 Deploy and Inference",
    "text": "5.8 Deploy and Inference\nThe EIS will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Go to the Deployment section, select Arduino Library, and at the bottom, choose Quantized (Int8) and press Build.\n\n\n\n\n\nWhen the Build button is selected, a zip file will be created and downloaded to your computer. On your Arduino IDE, go to the Sketch tab, select the option Add .ZIP Library, and Choose the .zip file downloaded by EIS:\n\n\n\n\n\nNow, it is time for a real test. We will make inferences while completely disconnected from the EIS. Let’s use the NiclaV code example created when we deployed the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab, look for your project, and select nicla-vision/nicla-vision_microphone (or nicla-vision_microphone_continuous)\n\n\n\n\n\nPress the reset button twice to put the NiclaV in boot mode, upload the sketch to your board, and test some real inferences:"
  },
  {
    "objectID": "kws_nicla.html#post-processing",
    "href": "kws_nicla.html#post-processing",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.9 Post-processing",
    "text": "5.9 Post-processing\nNow that we know the model is working since it detects our keywords, let’s modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that whenever the keyword YES is detected, the Green LED will light; if a NO is heard, the Red LED will light, if it is a UNKNOW, the Blue LED will light; and in the presence of noise (No Keyword), the LEDs will be OFF.\nWe should modify one of the code examples. Let’s do it now with the nicla-vision_microphone_continuous.\nStart with initializing the LEDs:\n...\nvoid setup()\n{\n        // Once you finish debugging your code, you can comment or delete the Serial part of the code\n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Nicla Vision KWS with LEDs\");\n    \n    // Pins for the built-in RGB LEDs on the Arduino NiclaV\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    pinMode(LEDB, OUTPUT);\n\n    // Ensure the LEDs are OFF by default.\n    // Note: The RGB LEDs on the Arduino Nicla Vision\n    // are ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n...\n}\nCreate two functions, turn_off_leds() function , to turn off all RGB LEDs\n**\n * @brief      turn_off_leds function - turn-off all RGB LEDs\n */\nvoid turn_off_leds(){\n    digitalWrite(LEDR, HIGH);\n    digitalWrite(LEDG, HIGH);\n    digitalWrite(LEDB, HIGH);\n}\nAnother turn_on_led() function is used to turn on the RGB LEDs according to the most probable result of the classifier.\n/**\n * @brief      turn_on_leds function used to turn on the RGB LEDs\n * @param[in]  pred_index     \n *             no:       [0] ==&gt; Red ON\n *             noise:    [1] ==&gt; ALL OFF \n *             unknown:  [2] ==&gt; Blue ON\n *             Yes:      [3] ==&gt; Green ON\n */\nvoid turn_on_leds(int pred_index) {\n  switch (pred_index)\n  {\n    case 0:\n      turn_off_leds();\n      digitalWrite(LEDR, LOW);\n      break;\n\n    case 1:\n      turn_off_leds();\n      break;\n    \n    case 2:\n      turn_off_leds();\n      digitalWrite(LEDB, LOW);\n      break;\n\n    case 3:\n      turn_off_leds();\n      digitalWrite(LEDG, LOW);\n      break;\n  }\n}\nAnd change the // print the predictions portion of the code on loop():\n...\n\n    if (++print_results &gt;= (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)) {\n        // print the predictions\n        ei_printf(\"Predictions \");\n        ei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n            result.timing.dsp, result.timing.classification, result.timing.anomaly);\n        ei_printf(\": \\n\");\n\n        int pred_index = 0;     // Initialize pred_index\n        float pred_value = 0;   // Initialize pred_value\n\n        for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n            if (result.classification[ix].value &gt; pred_value){\n                pred_index = ix;\n                pred_value = result.classification[ix].value;\n            }\n            // ei_printf(\"    %s: \", result.classification[ix].label);\n            // ei_printf_float(result.classification[ix].value);\n            // ei_printf(\"\\n\");\n        }\n        ei_printf(\"  PREDICTION: ==&gt; %s with probability %.2f\\n\", \n                  result.classification[pred_index].label, pred_value);\n        turn_on_leds (pred_index);\n\n        \n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n        ei_printf(\"    anomaly score: \");\n        ei_printf_float(result.anomaly);\n        ei_printf(\"\\n\");\n#endif\n\n        print_results = 0;\n    }\n}\n\n...\nYou can find the complete code on the project’s GitHub.\nUpload the sketch to your board and test some real inferences. The idea is that the Green LED will be ON whenever the keyword YES is detected, the Red will lit for a NO, and any other word will turn on the Blue LED. All the LEDs should be off if silence or background noise is present. Remember that the same procedure can “trigger” an external device to perform a desired action instead of turning on an LED, as we saw in the introduction."
  },
  {
    "objectID": "kws_nicla.html#conclusion",
    "href": "kws_nicla.html#conclusion",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.10 Conclusion",
    "text": "5.10 Conclusion\n\nYou will find the notebooks and codes used in this hands-on tutorial on the GitHub repository.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection, Gunshot)\nIndustry (Anomaly Detection)\nMedical (Snore, Cough, Pulmonary diseases)\nNature (Beehive control, insect sound, pouching mitigation)"
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#introduction",
    "href": "Motion_Classif_Anomaly_Detect.html#introduction",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers’ safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\n\n6.1.1 Learning Objectives\n\nSetting up the Arduino Nicla Vision Board\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\nConclusion and Next Steps\n\nBy the end of this tutorial, you’ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#imu-installation-and-testing",
    "href": "Motion_Classif_Anomaly_Detect.html#imu-installation-and-testing",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.2 IMU Installation and testing",
    "text": "6.2 IMU Installation and testing\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, Setup Nicla Vision, the Nicla Vision Board has an onboard 6-axis IMU: 3D gyroscope and 3D accelerometer, the LSM6DSOX. Let’s verify if the LSM6DSOX IMU library is installed. If not, install it.\n\n\n\n\n\nNext, go to Examples &gt; Arduino_LSM6DSOX &gt; SimpleAccelerometer and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n\n\n\n\n\n\n6.2.1 Defining the Sampling frequency:\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you’re interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\nNature of the Motion: Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\nHardware Limitations: The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\nComputational Resources: Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\nBattery Life: A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\nData Storage: More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, sampling rates of around 50 Hz to 100 Hz are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\nLet’s define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50Hz):\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include &lt;Arduino_LSM6DSOX.h&gt;\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n  }\n}\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n\n\n\n\n\n\nNote that with the Nicla board resting on a table (with the camera facing down), the z-axis measures around 9.8m/s\\(^2\\), the expected earth acceleration."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#the-case-study-simulated-container-transportation",
    "href": "Motion_Classif_Anomaly_Detect.html#the-case-study-simulated-container-transportation",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.3 The Case Study: Simulated Container Transportation",
    "text": "6.3 The Case Study: Simulated Container Transportation\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we’ll capture motion data by manually simulating the conditions of:\n\nTerrestrial Transportation (by road or train)\nMaritime-associated Transportation\nVertical Movement via Fork-Lift\nStationary (Idle) period in a Warehouse\n\n\n\n\n\n\nFrom the above images, we can define for our simulation that primarily horizontal movements (x or y axis) should be associated with the “Terrestrial class,” Vertical movements (z-axis) with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#data-collection",
    "href": "Motion_Classif_Anomaly_Detect.html#data-collection",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.4 Data Collection",
    "text": "6.4 Data Collection\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the CSV Wizard tool.\n\nIn this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\n\n6.4.1 Connecting the device to Edge Impulse\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\nDownload the latest firmware and connect it directly to the Data Collection section.\nUse the CLI Data Forwarder tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the Setup Nicla Vision hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let’s do it with the last one.\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\nInstall the Edge Impulse CLI and the Node.js into your computer.\nUpload a sketch for data capture (the one discussed previously in this tutorial).\nUse the CLI Data Forwarder to capture data from the Nicla’s accelerometer and send it to the Studio, as shown in this diagram:\n\n\n\n\n\n\nStart the CLI Data Forwarder on your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables (for example, accX, accY, and accZ), and device name (for example, NiclaV:\n\n\n\n\n\nGo to the Devices section on your EI Project and verify if the device is connected (the dot should be green):\n\n\n\n\n\n\nYou can clone the project developed for this hands-on: NICLA Vision Movement Classification.\n\n\n\n6.4.2 Data Collection\nOn the Data Acquisition section, you should see that your board [NiclaV] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50Hz]. The Studio suggests a sample length of [10000] ms (10s). The last thing left is defining the sample label. Let’s start with[terrestrial]:\n\n\n\n\n\nTerrestrial (palettes in a Truck or Train), moving horizontally. Press [Start Sample]and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n\n\n\n\n\nAs expected, the movement was captured mainly in the Y-axis (green). In the blue, we see the Z axis, around -10 m/s\\(^2\\) (the Nicla has the camera facing up).\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\nMaritime (pallets in boats into an angry ocean). The movement is captured on all three axes:\n\n\n\n\n\nLift (Palettes being handled vertically by a Forklift). Movement captured only in the Z-axis:\n\n\n\n\n\nIdle (Paletts in a warehouse). No movement detected by the accelerometer:\n\n\n\n\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the three dots menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab. Below, you can see the resulting dataset:\n\n\n\n\n\nOnce you have captured your dataset, you can explore it in more detail using the Data Explorer, a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as PCA or t-SNE to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n\n\n\n\n\nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only “vertical”."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#impulse-design",
    "href": "Motion_Classif_Anomaly_Detect.html#impulse-design",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.5 Impulse Design",
    "text": "6.5 Impulse Design\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a learning block to classify new data. Go to Impulse Design and Create Impulse. The Studio will suggest the basic design. Let’s also add a second Learning Block for Anomaly Detection.\n\n\n\n\n\nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n\n\n\n\n\nThe sampling frequency should be automatically captured, if not, enter it: [50]Hz. The Studio suggests a Window Size of 2 seconds ([2000] ms) with a sliding window of [20]ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n\n\n\n\n\nLet’s dig into those steps and parameters to understand better what we are doing here.\n\n6.5.1 Data Pre-Processing Overview\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations.\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or windows. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window increase) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of “cycles of data”.\n\nWith a sampling rate (SR) of 50Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis x 2 seconds x 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n\n\n\n\n\nOnce the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n\n6.5.2 EI Studio Spectral Features\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block.\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as FFT or Wavelets.\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of [32].\nThe per axis/channel Time Domain Statistical features are:\n\nRMS: 1 feature\nSkewness: 1 feature\nKurtosis: 1 feature\n\nThe per axis/channel Frequency Domain Spectral features are:\n\nSpectral Power: 16 features (FFT Length/2)\nSkewness: 1 feature\nKurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\nYou can learn more about how each feature is calculated by downloading the notebook Edge Impulse - Spectral Features Block Analysis TinyML under the hood: Spectral Analysis or opening it directly on Google CoLab.\n\n\n\n6.5.3 Generating features\nOnce we understand what the pre-processing does, it is time to finish the job. So, let’s take the raw data (time-series type) and convert it to tabular data. For that, go to the Spectral Features section on the Parameters tab, define the main parameters as discussed in the previous section ([FFT] with [32] points), and select[Save Parameters]:\n\n\n\n\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#models-training",
    "href": "Motion_Classif_Anomaly_Detect.html#models-training",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.6 Models Training",
    "text": "6.6 Models Training\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\n\n\n\n\nAs hyperparameters, we will use a Learning Rate of [0.005], a Batch size of [32], and [20]% of data for validation for [30] epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n\n\n\n\n\nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be [32], as suggested by the Studio:"
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#testing",
    "href": "Motion_Classif_Anomaly_Detect.html#testing",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.7 Testing",
    "text": "6.7 Testing\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\nThe default minimum threshold for a considered uncertain result is [0.6] for classification and [0.3] for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can Set confidence thresholds on the three dots menu, besides the Classy all button.\n\n\n\n\n\nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device)."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#deploy",
    "href": "Motion_Classif_Anomaly_Detect.html#deploy",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.8 Deploy",
    "text": "6.8 Deploy\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, you can choose Quantized (Int8) or Unoptimized (float32) and [Build]. A Zip file will be created and downloaded to your computer.\n\n\n\n\n\nOn your Arduino IDE, go to the Sketch tab, select Add.ZIP Library, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: Library installed.\n\n6.8.1 Inference\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select Nicla_vision_fusion:\n\n\n\n\n\nNote that the code created by Edge Impulse considers a sensor fusion approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n/* Includes ---------------------------------------------------------------- */\n#include &lt;NICLA_Vision_Movement_Classification_inferencing.h&gt; \n#include &lt;Arduino_LSM6DSOX.h&gt; //IMU\n#include \"VL53L1X.h\" // ToF\n\nYou can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla [RESET] button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\nIdle and lift classes:\n\n\n\n\n\n\n\nmaritime and terrestrial:\n\n\n\n\n\n\nNote that in all situations above, the value of the anomaly score was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, “rolling” the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\nanomaly detection:\n\n\n\n\n\n\nIn this case, the anomaly is much bigger, over 1.00\n\n\n6.8.2 Post-processing\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if terrestrial is detected, the Green LED will light; if maritime, the Red LED will light, if it is a lift, the Blue LED will light; and if no movement is detected (idle), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously)."
  },
  {
    "objectID": "Motion_Classif_Anomaly_Detect.html#conclusion",
    "href": "Motion_Classif_Anomaly_Detect.html#conclusion",
    "title": "6  Motion Classification and Anomaly Detection",
    "section": "6.9 Conclusion",
    "text": "6.9 Conclusion\n\nThe notebooks and codes used in this hands-on tutorial will be found on the GitHub repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\n6.9.1 Case Applications\n\n6.9.1.1 Industrial and Manufacturing\n\nPredictive Maintenance: Detecting anomalies in machinery motion to predict failures before they occur.\nQuality Control: Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\nWarehouse Logistics: Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n\n\n6.9.1.2 Healthcare\n\nPatient Monitoring: Detecting falls or abnormal movements in the elderly or those with mobility issues.\nRehabilitation: Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\nActivity Recognition: Classifying types of physical activity for fitness applications or patient monitoring.\n\n\n\n6.9.1.3 Consumer Electronics\n\nGesture Control: Interpreting specific motions to control devices, such as turning on lights with a hand wave.\nGaming: Enhancing gaming experiences with motion-controlled inputs.\n\n\n\n6.9.1.4 Transportation and Logistics\n\nVehicle Telematics: Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\nCargo Monitoring: Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n\n\n6.9.1.5 Smart Cities and Infrastructure\n\nStructural Health Monitoring: Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\nTraffic Management: Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n\n\n6.9.1.6 Security and Surveillance\n\nIntruder Detection: Detecting motion patterns typical of unauthorized access or other security breaches.\nWildlife Monitoring: Detecting poachers or abnormal animal movements in protected areas.\n\n\n\n6.9.1.7 Agriculture\n\nEquipment Monitoring: Tracking the performance and usage of agricultural machinery.\nAnimal Behavior Analysis: Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n\n\n6.9.1.8 Environmental Monitoring\n\nSeismic Activity: Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\nOceanography: Studying wave patterns or marine movements for research and safety purposes.\n\n\n\n\n6.9.2 Nicla 3D case\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: Arduino Nicla Voice and Vision Wearable Case.\n\n\n\n\n\nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nWALC 2023 Applied AI Track is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis by Wes McKinney”\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden, Daniel Situnayake\n“TinyML Cookbook” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake, Jenny Plunkett\n“MACHINE LEARNING SYSTEMS for TinyML” Collaborative effort\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  }
]